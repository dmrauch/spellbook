{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Stroke Prediction - Model Training and Validation\n\nNow that the inputs are prepared, we can proceed to set up and configure a\nmodel. I tried different configurations with up to three hidden layers with\ndifferent numbers of nodes in each layer and eventually settled on the following\nmodel layout:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   model = tf.keras.models.Sequential([\n      tf.keras.layers.InputLayer(input_shape=(10,)),\n      tf.keras.layers.Dense(10, activation='relu'),\n      tf.keras.layers.Dense(100, activation='relu',\n            kernel_regularizer=tf.keras.regularizers.L2(l2=0.001)),\n      tf.keras.layers.Dropout(rate=0.2),\n      tf.keras.layers.Dense(50, activation='relu'),\n      tf.keras.layers.Dropout(rate=0.2),\n      tf.keras.layers.Dense(1, activation='sigmoid')\n   ])\n   model.summary()\n\nThis sets up a neural network with three fully connected hidden layers,\ninterleaved with two dropout layers and with :term:`L2 regularisation`\nactivated in the middle hidden layer.\n\nSince there are 10 input variables, the input layer of the network has\n``input_shape=(10,)``. The following dense layers have 10, 100 and 50 neurons,\nrespectively. Dropout is a regularisation technique\naimed at eliminating overtraining by introducing an element of randomness.\nDuring training, a :term:`dropout` layer randomly discards a configurable\nfraction of the inputs received from the preceding layer during each\nfeed-forward step. In this case, 20% of the previous layer's outputs are thrown\naway by each of the two dropout layers. Finally, the network ends with a single\nsigmoid-activated output neuron. This is a typical situation in binary\nclassification - low values near 0 denote one class, larger values near 1\nthe other.\n\nThis model has a total of 6311 parameters as can be seen from the ``summary()``\ncommand:\n\n- The first hidden dense layer with 10 neurons, which are fed from the 10\n  input nodes plus a bias node, has (10 + 1) \u00b7 10 = 110 parameters\n- The second dense layer with 100 neurons, which are fed from the 10 neurons\n  of the previous layer plus a bias node, has (10 + 1) \u00b7 100 = 1100\n  parameters\n- Dropout layers do not have any free parameters that can be optimised during\n  training\n- Similarly, the third dense layer and the single-neuron output layer have\n  5050 and 51 parameters, respectively.\n\nBefore we can begin training the model, we need to equip the model with a\nfew more tools, in particular, a *loss function* and an *optimiser*.\nMetrics that quantify certain aspects of the model's performance are not\nstrictly necessary but highly useful:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n   in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   model.compile(loss='binary_crossentropy',\n                 optimizer='adam',\n                 metrics=[\n                     tf.keras.metrics.BinaryCrossentropy(name='binary_crossentropy'),\n                     tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n                     tf.keras.metrics.TruePositives(name='tp'),\n                     tf.keras.metrics.TrueNegatives(name='tn'),\n                     tf.keras.metrics.FalsePositives(name='fp'),\n                     tf.keras.metrics.FalseNegatives(name='fn'),\n                     tf.keras.metrics.Recall(name='recall'),\n                     tf.keras.metrics.Precision(name='precision')\n                 ])\n\nThe optimiser is the algorithm responsible for minimising the loss\nfunction and the loss function quantifies how much the predictions of the\nmodel generated during training differ from the training labels.\nThe loss function of choice for binary classification tasks is\n*binary crossentropy* (:class:`tf.keras.losses.BinaryCrossentropy`)\nand *Adam* (:class:`tf.keras.optimizers.Adam`) is one of several optimisers\nthat can be used.\n\nIn binary classification, in particular, there is a wide range of different,\nbut related metrics. Let us take this opportunity and briefly discuss some of\nthe metrics specifically geared towards binary classification - a thorough\ndiscussion can be found on `Wikipedia: Sensitivity and specificity\n<https://en.wikipedia.org/wiki/Sensitivity_and_specificity>`_:\n\n- **True Positives** (:term:`TP`): The number of datapoints that were classified\n  as belonging to the *positive* class, corresponding to the presence of a\n  stroke in our example, by the model and that in reality do belong to the\n  positive class\n- **True Negatives** (:term:`TN`): The number of datapoints that were classified\n  as belonging to the *negative* class, corresponding to the absence of a\n  stroke in our example, by the model and that in reality do belong to the\n  negative class\n- **False Positives** (:term:`FP`): The number of datapoints that were\n  classified as belonging to the *positive* class but truly belong to the\n  *negative* class\n- **False Negatives** (:term:`FN`): The number of datapoints that were\n  classified as belonging to the *negative* class but truly belong to the\n  *positive* class\n- **Recall**, also called *sensitivity* and *True Positive Rate*\n  (:term:`TPR`) is defined as\n\n  .. math:: \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\n  and quantifies the fraction of the truly *positve* datapoints that were\n  correctly predicted/classified by the model.\n- **Precision** is defined as\n\n  .. math:: \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\n  and quantifies the fraction of all positively classified datapoints that are\n  in fact truly positive.\n- **Accuracy**: The fraction of datapoints that were correctly\n  classified, i.e. the predicted class was identical to the true class.\n  Accuracy is not a specifically binary metrics but is also very useful in\n  multi-class classification problems.\n\nFinally, we start the training as follows:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n   in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   epochs = 100\n   history = model.fit(train, epochs=epochs,\n       validation_data=val,\n       callbacks = [\n           tf.keras.callbacks.CSVLogger(\n               filename='naive-e{}-history.csv'.format(epochs)),\n           sb.train.ModelSavingCallback(\n               foldername='model-naive-e{}'.format(epochs))\n       ])\n\nThis configures 100 training epochs, during each of which the full training\nset is digested and processed by the network in its search for ever-improving\nmodel parameters. During training not only the loss is calculated, but also\nthe values of the other quantities specified in the metrics list. At the end\nof each epoch, the complete validation dataset is consumed to calculate the\nsame metrics on a disjoint dataset that is not used for adjusting the model\nparameters. The values of all metrics, both those calculated from the training\nset and those evaluated from the test set, are written to a ``*.csv`` file by\nthe :class:`tf.keras.callbacks.CSVLogger` callback at the end of each epoch.\nSimilarly, the :class:`spellbook.train.ModelSavingCallback` callback saves\nthe entire model including its architecture as well as all parameter values\nevery 10 epochs. This can be very handy because it preserves the model and\nits state in case the training has to be cancelled.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>I will use this exact model setup in the rest of this\n          tutorial to demonstrate the effect that different training strategies\n          can have on models with exactly the same layout. The only difference\n          will be in the batch size, which is taken to be 100 here in the\n          first approach and is reduced to the default value of 32 in the\n          later versions.</p></div>\n\n\n## Naive Approach\n\nLet's have a look at the training and validation metrics we obtain when\ntraining the model in this 'naive' setup using the imbalanced dataset with\n96% negative (*no stroke*) and 4% positive (*stroke*) cases.\n\nPlots showing the evolution of the binary metrics can be generated with\n*spellbook*'s :func:`spellbook.train.plot_history_binary` function:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n   in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # inspect/plot the training history\n   sb.train.plot_history_binary(history,\n      '{}-e{}-history'.format(prefix, epochs))\n\nThe resulting plot will be shown in Figure 24.\n\nIn order to determine the confusion matrix, it is necessary to retrieve the\npredictions of the trained model for the validation data. These can be\nobtained after some reorganisation and type conversion gymnastics:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n   in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # separate the datasets into features and labels\n   _, train_labels = sb.input.separate_tfdataset_to_tftensors(train)\n   _, val_labels = sb.input.separate_tfdataset_to_tftensors(val)\n\n   # obtain the predictions of the model\n   train_predictions = model.predict(train)\n   val_predictions = model.predict(val)\n   \n   # not strictly necessary: remove the superfluous inner nesting\n   train_predictions = tf.reshape(train_predictions, train_predictions.shape[0])\n   val_predictions = tf.reshape(val_predictions, val_predictions.shape[0])\n   \n   # until now the predictions are still continuous in [0, 1] and this breaks the\n   # calculation of the confusion matrix, so we need to set them to either 0 or 1\n   # according to a default intermediate threshold of 0.5\n   train_predicted_labels = sb.train.get_binary_labels(\n       train_predictions, threshold=0.5)\n   val_predicted_labels = sb.train.get_binary_labels(\n       val_predictions, threshold=0.5)\n   \n   # calculate and plot the confusion matrix\n   class_names = list(categories['stroke'].values())\n   class_ids = list(categories['stroke'].keys())\n   val_confusion_matrix = tf.math.confusion_matrix(val_labels,\n                                                   val_predicted_labels,\n                                                   num_classes=len(class_names))\n\nThe confusion matrix relates the predicted classes to the true classes and\nshows how often the model was accurate and in what way it erred. It can\nbe plotted with the :func:`spellbook.plot.plot_confusion_matrix` function:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n   in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # absolute datapoint counts\n   sb.plot.save(\n      sb.plot.plot_confusion_matrix(\n         confusion_matrix = val_confusion_matrix,\n         class_names = class_names,\n         class_ids = class_ids),\n      filename = '{}-e{}-confusion.png'.format(prefix, epochs))\n\nThe resulting plot is shown in Figure 25.\n\n\n.. list-table::\n   :class: spellbook-gallery-wrap\n\n   * - .. figure:: /images/examples/1-binary-stroke-prediction/naive-e100-history-loss-acc.png\n          :height: 300px\n\n          Figure 24: Evolution of the loss and the classification accuracy\n          during training in the *naive* approach\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/naive-e100-confusion.png\n          :height: 300px\n\n          Figure 25: Confusion matrix with absolute datapoint counts in the\n          *naive* approach\n\nAs we can see from Figure 24, the loss improves only in the beginning, but\nthen only decreases very slightly after about 10 epochs. Similarly, the\naccuracy reaches 95-96% at about the same time and does not improve beyond\nthat. This accuracy very closely resembles the fractions of the two target\nclasses or labels. Recall from `Figure 1 <ex1-fig-variables>` in\n:doc:`/examples/1-binary-stroke-prediction/index` that 95.7% of the\npeople in the dataset have not had a stroke\ncompared to 4.3% who have. Looking at the confusion matrix in\n`Figure 25 <ex1-fig-naive>`, we\ncan confirm what has happened: The model has learned to practically only and\nexclusively categorise the data as *negative* or *healthy*. Since it predicts\nall datapoints to be *negative*, its accuracy simply reflects how often this\ncategory appears in the dataset and despite the 95% accuracy it is in fact\ncompletely ignorant. Looking at the evolution of the true/false positive/\nnegative counts as well as the precision and recall in Figures 26 and 27,\nwe can indeed confirm that this is not an accidental fluctuation:\n\n.. list-table::\n   :class: spellbook-gallery-wrap\n\n   * - .. figure:: /images/examples/1-binary-stroke-prediction/naive-e100-history-pos-neg.png\n          :height: 300px\n\n          Figure 26: Evolution of the true/false positive/negative counts\n          during training in the *naive* approach\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/naive-e100-history-rec-prec.png\n          :height: 300px\n\n          Figure 27: Evolution of precision and recall during training\n          in the *naive* approach\n\nAll of this is despite the fact that the batch size was increased to 100\nso as to make it probable that at least *some* positive cases were present in\neach batch. However, the 4% fraction of the positive cases is simply just\nmuch smaller than the 96% fraction of negative cases.\n\nThis is a typical situation when dealing with *imbalanced data*, where one\ntarget class appears substantially more often than the other(s). Note that\nsimply training longer does not change the fundamental challenge. Instead, when\nfacing this situation, two approaches come to mind:\n\n- **Class/event weights**: Assigning larger weights to the minority events, thus\n  making them more 'important' effectively. This is similar to the *boosting*\n  approach commonly used when training sets of decision trees (even in balanced\n  situations).\n- **Oversampling**: Balancing the datasets by passing the same minority events\n  to the model multiple times during each epoch\n\nIn this tutorial, we will apply the oversampling method.\n\n\n\n## Oversampling the Minority Class\n\nThe idea of oversampling is to divide the dataset into two parts according the\npositive and\nnegative cases, thus separating the majority and minority classes and then to\nrepeatedly sample with replacement from the minority classes to create a\n'pseudo' minority dataset that is equal in size to the majority class. This\nresampled minority dataset will naturally contain events multiple times and the\noversampling factor is given by the ratio of the sizes of the majority and the\noriginal minority sets. Oversampling is implemented in\n:func:`spellbook.input.oversample` and can be applied as follows:\n\n.. margin:: from **3-stroke-prediction-oversampling.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # oversampling (including shuffling of the data)\n   data = sb.input.oversample(data, target)\n\nUnder the hood, oversampling is realised as follows:\n\n.. margin:: from :func:`spellbook.input.oversample`\n\n    in :mod:`spellbook.input`\n\n.. code:: python\n\n   # get the different categories\n   cats = data[target].cat.categories\n\n   # split data by target category\n   datapoints = {}\n   for i in range(len(cats)):\n       datapoints[cats[i]] = data[data[target] == cats[i]]\n\n   # count datapoints in each category\n   counts = np.zeros(shape = len(cats), dtype=int)\n   for i in range(len(cats)):\n       counts[i] = datapoints[cats[i]][target].count()\n\n   # get number of datapoints in the largest category\n   nmax = np.amax(counts)\n\n   resampled = pd.DataFrame()\n   for i, cat in enumerate(cats):\n\n       # how many datapoints are missing?\n       nmiss = nmax - len(datapoints[cat])\n       assert nmiss >= 0\n\n       if nmiss > 0:\n\n           # ordered array of indices of the underrepresented category\n           indices = np.arange(counts[i])\n\n           # draw nmiss times with replacement from the indices array\n           choices = np.random.choice(indices, size=nmiss)\n\n           datapoints[cat] = datapoints[cat].append(\n               datapoints[cat].iloc[choices])\n\n       resampled = resampled.append(datapoints[cat])\n   \n   if shuffle: resampled = resampled.sample(frac = 1.0)\n\nThe data is grouped and separated according to the different categories of\nthe target variable and the difference in size between the largest\nclass/category and the minority classes/categories is made up by sampling\nthem with replacement until the size gaps are closed. This way, it is ensured\nthat all original datapoints in the minority classes are preserved and only\nthe datapoints needed to fill the gaps are randomly sampled.\n\n.. list-table::\n   :class: spellbook-gallery-wrap\n\n   * - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-e500-history-loss-acc.png\n          :height: 300px\n\n          Figure 28: Evolution of the loss and the classification accuracy\n          during training when *oversampling* is used\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-e500-history-pos-neg.png\n          :height: 300px\n\n          Figure 29: Evolution of the true/false positives/negatives during training when *oversampling* is used\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-e500-confusion-norm-true.png\n          :height: 300px\n\n          Figure 30: Confusion matrix, normalised for each true target class\n\nIn Figure 28 we can see that the loss is now somewhat increased compared to\nthe *naive* approach, but decreases steadily over a longer range of training\nepochs. Likewise, the initial classification accuracy has decreased to about\n70% and continually improves from there to about 86%. Given that the training\nand validation sets now, thanks to oversampling, are made up of very similar\namounts of *positive* and *negative* datapoints, this accuracy is\nactually meaningful. This can also be seen in Figures 29 and 30, which show\nthe evolution of the numbers of true/false positives/negatives and the\ntruth-normalised confusion matrix, respectively. Now, the model is actually\nlearning to detect *positive* cases and reaches a performance with 0.5% of\nthe truly *positive* cases wrongly classified as *negative* and 27.6% of the\ntruly *negative* cases wrongly classified as *positive*.\n\n\n\n## Normalising the Inputs\n\nNow, we are going to apply an additional technique on top -\n*input normalisation*. Normally, this should be done earlier, but I wanted to\nevaluate the benefit this brings by benchmarking against the previous\nmodel based on un-normalised inputs. The strategy of input normalisation is\nbased on the sensitivity of neural networks to the scale of numerical\ninput values - variables of the order of magnitude 1 are handled much better\nthan values in the range of hundreds, thousands or even larger.\n\nOne common approach is *standardisation*, where a variable is shifted and\nscaled so that after the transformation its mean is 0 and its variance 1.\nHere, in order to see how powerful even a very simplistic normalisation is,\nwe are just dividing the continuous numerical variables by some constant\nfactors to rescale them to intervals roughly between 0 and 1:\n\n.. margin:: from **4-stroke-prediction-oversampling-norm.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # normalisation\n   data['age_norm'] = data['age'] / 100.0\n   data['avg_glucose_level_norm'] = data['avg_glucose_level'] / 300.0\n   data['bmi_norm'] = data['bmi'] / 100.0\n   # replace unnormalised variable names with their normalised counterparts\n   features[features.index('age')] = 'age_norm'\n   features[features.index('avg_glucose_level')] = 'avg_glucose_level_norm'\n   features[features.index('bmi')] = 'bmi_norm'\n\nWe are keeping the original unnormalised variables, created new, normalised\nones with the suffix ``_norm`` and replace the original variables with them\nin the list of feature variables which is later used for splitting the full\ndataset into separate sets for the features and the labels.\n\nAfter training for 2000 epochs, we get the following results:\n\n.. list-table::\n   :class: spellbook-gallery-wrap\n\n   * - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-normalised-e2000-history-loss-acc.png\n          :height: 300px\n\n          Figure 31: Evolution of the loss and the classification accuracy\n          during training when both *oversampling* and *input normalisation*\n          are used\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-normalised-e2000-history-pos-neg.png\n          :height: 300px\n\n          Figure 32: Evolution of the true/false positives/negatives\n          during training when both *oversampling* and *input normalisation*\n          are used\n\n     - .. figure:: /images/examples/1-binary-stroke-prediction/oversampling-normalised-e2000-confusion-norm-true.png\n          :height: 300px\n\n          Figure 33: Confusion matrix, normalised for each true target class\n\nAs we can see from Figure 33, the performance of the model has improved further\nas reflected in reduced classification errors shown in the off-diagonal\ncells of the confusion matrix: The fraction of true negatives wrongly\nclassified as positive has decreased to 9.5% and the fraction of true positives\nwrongly classified as negative has even gone down to zero.\n\n\n\n## Comparison: ROC Curves\n\nFinally, let's compare all three approaches in yet another way: by means of\nthe *Receiver Operator Characteristic* (:term:`ROC`) curves.\nROC curves are a commonly used tool to benchmark the performance of different\nmodels against each other. They show the *False Positive Rate* (:term:`FPR`)\non the x-axis, indicating how often truly negative datapoints are\nwrongly classified as positive, and the *True Positive Rate* (:term:`TPR`) on\nthe y-axis, indicating what fraction of the truly positive datapoints are\ncorrectly classified.\n\nROC curves are implemented in :class:`spellbook.train.ROCPlot`.\nAt the end of each training, we determine the ROC curves for the respective\nmodels and save them to disk using *Python*'s *pickle* mechanism:\n\n.. margin:: from **4-stroke-prediction-oversampling-norm.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   # calculate and plot the ROC curve\n   roc = sb.train.ROCPlot()\n   roc.add_curve('{} / {} epochs (training)'.format(name, epochs),\n       train_labels, train_predictions.numpy(),\n       plot_args = dict(color='C0', linestyle='--'))\n   roc.add_curve('{} / {} epochs (validation)'.format(name, epochs),\n       val_labels, val_predictions.numpy(),\n       plot_args = dict(color='C0', linestyle='-'))\n   sb.plot.save(roc.plot(), '{}-e{}-roc.png'.format(prefix, epochs))\n   roc.pickle_save('{}-e{}-roc.pickle'.format(prefix, epochs))\n\nWe can subsequently load these pickle files with\n\n.. margin:: from **5-roc.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   roc_naive100 = ROCPlot.pickle_load('naive-e100-roc.pickle')\n   roc_oversampling2000 = ROCPlot.pickle_load('oversampling-e2000-roc.pickle')\n   roc_norm2000 = ROCPlot.pickle_load('oversampling-normalised-e2000-roc.pickle')\n\nand combine the ROC curves of different models in a single plot with\n\n.. margin:: from **5-roc.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n   roc = ROCPlot()\n   roc += roc_naive100\n   WP = roc.get_WP('naive / 100 epochs (validation)', threshold=0.5)\n   roc.draw_WP(WP, linestyle='-', linecolor='C1')\n   roc.curves['naive / 100 epochs (training)']['line'].set_color('C1')\n   roc.curves['naive / 100 epochs (validation)']['line'].set_color('C1')\n   \n   roc += roc_oversampling2000\n   WP = roc.get_WP('oversampling / 2000 epochs (validation)', threshold=0.5)\n   roc.draw_WP(WP, linestyle='-', linecolor='C0')\n   \n   roc += roc_norm2000\n   WP = roc.get_WP('oversampling normalised / 2000 epochs (validation)', threshold=0.5)\n   roc.draw_WP(WP, linestyle='-', linecolor='black')\n   roc.curves['oversampling normalised / 2000 epochs (training)']['line'].set_color('black')\n   roc.curves['oversampling normalised / 2000 epochs (validation)']['line'].set_color('black')\n\n   sb.plot.save(roc.plot(), prefix+'roc-2000-naive-oversampling-normalised.png')\n\nThis code also serves to draw specific working points on the ROC curves,\ncorresponding to picking the values of the model output (the sigmoid-activated\noutput of the last model layer) that mark the boundary between classifying\na datapoint as *negative* or as *positive*. Defining different working points\nwith different threshold values can decrease the number of false negatives\nat the cost of increasing the number of false positives and vice versa.\nHere, we are simply going to stick to the default working points with\nthreshold values of 0.5.\n\nIn order to have a fair benchmark, we plot the ROC curves for the second and\nthird model after training for 2000 epochs in both cases. Since it was\nobvious that the first, *naive* model does not perform well at all for\nsystematic reasons and that this does not change when training longer, we\nstick to 100 training epochs for this model and only include it for the\nsake of completeness.\n\n.. figure:: /images/examples/1-binary-stroke-prediction/roc-2000-naive-oversampling-normalised.png\n   :align: center\n   :height: 450px\n\nWe can see that the third model, trained with both *oversampling* and\n*input normalisatoin* outperforms the other two, as indicated by its ROC\ncurve extending further to the top left corner of the plot, corresponding\nto lower FPR and higher TPR. The *area under the curve* (:term:`AUC`) metric\ncondenses this into a single number and we can see that the third model\nreaches an AUC of about 0.98 on the validation set as opposed to 0.94 for the\nsecond model.\n\n\n\n## Summary\n\nIn this tutorial, we used *TensorFlow* to set up and train a neural network\nfor binary classification, detecting whether or not patients were suffering\nfrom a stroke. We encountered the problem of *imbalanced data*, saw how it\nprevented a first naive model from learning to distinguish between both\ntarget classes and then used *oversampling* to train a better classifier.\nWe also saw the impact of *input normalisation* and used different metics\nas well as ROC curves to compare the performance of our different models.\nOur final classifier reached an AUC of about 0.98 with a TPR of 100% and an FPR\nof about 11% for the default working point with a treshold of 0.5 on the\nsigmoid-activated output of the single node in the last network layer.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}