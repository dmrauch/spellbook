{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Stroke Prediction - Data Preparation and Input Pipeline\n\n\nNow let's move on to bringing the data into a form that can be processed by\nthe neural network!\n\nWe'll have a look at the three scripts\n\n- ``2-stroke-prediction-naive.py``\n- ``3-stroke-prediction-oversampling.py``\n- ``4-stroke-prediction-oversampling-norm.py``\n\nin ``examples/1-binary-stroke-prediction/``.\n\n\n\n## Treatment of Categorical Variables\n\n\nAfter data loading and cleaning, we start off by processing the categorical\nvariables and manipulating them so that they can be digested by the network.\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    # inplace convert string category labels to numerical indices\n    categories = sb.input.encode_categories(data)\n\nBehind the scenes, the function :func:`spellbook.input.encode_categories`\nloops over all categorical variables in the dataset and converts them to\n:class:`pandas.Categorical`\\s.\nA dictionary containing the mapping of the category names to numerical indices\nfor each categorical variable is returned::\n\n    categories: {\n        'gender': {0: 'female', 1: 'male', 2: 'other'},\n        'hypertension': {0: 'no', 1: 'yes'},\n        'heart_disease': {0: 'no', 1: 'yes'},\n        'ever_married': {0: 'no', 1: 'yes'},\n        'work_type': {0: 'children', 1: 'govt', 2: 'never', 3: 'private', 4: 'self'},\n        'residence_type': {0: 'rural', 1: 'urban'},\n        'smoking_status': {0: 'formerly', 1: 'never', 2: 'smokes', 3: 'unknown'},\n        'stroke': {0: 'no', 1: 'yes'}\n    }\n\nFinally, for each categorical\nvariable ``var``, an additional column ``var_codes`` is added to the dataset,\ncontaining the numerical category codes for each datapoint. These are the \ncolumns that we will later feed into the network.\n\nThe corresponding code in :func:`spellbook.input.encode_categories` looks\nlike this:\n\n.. margin:: from :func:`spellbook.input.encode_categories`\n\n    in :mod:`spellbook.input`\n\n.. code:: python\n\n    categories = {}\n    for var in data:\n        if data[var].dtype == 'object':\n            print(\"variable '{}' is categorical\".format(var))\n            data[var] = pd.Categorical(data[var])\n\n            # taken from https://stackoverflow.com/a/51102402\n            categories[var] = dict(enumerate(data[var].cat.categories))\n\n            # use numerical values instead of strings\n            data[var+'_codes'] = data[var].cat.codes\n\nNext, we are going to shuffle the dataset and order the rows randomly.\nAs we could see before, the original dataset is actually ordered in a way that\ndatapoints for patients with strokes come first. This is not what we want here\nbecause when splitting the dataset into a training and a validation set, this\nwould mean that stroke cases would only be present in the training but not\nthe validation set. We do the shuffling with :meth:`pandas.DataFrame.sample`\nand afterwards continue to adjust our list of the feature variables and the\nname of the target variable to point to the new columns holding the integer\ncategory indices:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    # shuffle data either now in pandas or later in TensorFlow\n    data = data.sample(frac = 1.0)\n\n    # use new numerical columns for the features\n    for var in categories:\n        if var == target:\n            target = target + '_codes'\n        else:\n            features[features.index(var)] = var + '_codes'\n\n\n\n## Feeding the Dataset into *TensorFlow*\n\nNow it is time to split the data into a *training set*, which is used to\nadjust the model parameters so as to describe the datapoints with\n:meth:`tf.keras.Model.fit`, and a *validation set*. The latter is used to\nevaluate the model performance and benchmark different models against each\nother when changing model :term:`hyperparameter`\\s such as the number of\nlayers, the number of nodes in a layer or the number of training epochs.\n\nWhile doing the split, we are also going to convert the training and validation\ndatasets into objects that can be fed into the network. There are at least\nthree different ways of doing this in terms of the objects and datatypes\ninvolved:\n\n#. from a :class:`pandas.DataFrame` to a :class:`tensorflow.data.Dataset`\n   where features and labels are combined in one object\n#. from a :class:`pandas.DataFrame` to two :class:`tensorflow.Tensor`\\s,\n   one for the features and one for the labels\n#. from a :class:`pandas.DataFrame` to two :class:`numpy.ndarray`\\s,\n   one for the features and one for the labels\n\nSince columns in a :class:`pandas.DataFrame` can be accessed in much the\nsame way as entries in a *Python* :class:`dict`, it is actually also possible\nto directly feed features stored in a :class:`pandas.DataFrame` and labels\nstored in a :class:`pandas.Series` into *TensorFlow* networks.\nThe separation of a single :class:`pandas.DataFrame` into separate feature\nand label sets for training, validation and testing is implemented in\n:func:`spellbook.input.split_pddataframe_to_pddataframes`.\n\n\n\n### Option 1: Using *TensorFlow* Datasets\n\nThis approach is taken in ``2-stroke-prediction-naive.py``.\nIt is implemented in :func:`spellbook.input.split_pddataframe_to_tfdatasets`\nand can be used like this:\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    train, val = sb.input.split_pddataframe_to_tfdatasets(data, target, features, n_train=3500)\n    print(train.cardinality()) # print the size of the dataset\n    print(val.cardinality())\n\nUsing 3500 datapoints for the training set and the remaining 1409 for the\nvalidation set corresponds to reserving a fraction of 71.3% of all data for\ntraining. A typical recommendation is to use about 70% for training when\ndealing with datasets containing a few thousand datapoints. As the size of\nthe total dataset increases, the fraction\nreserved for training can increase and when a million datapoints are available,\nit is perhaps sufficient to only use about 1% of them for validation.\nBasically, the logic behind this is to use as many datapoints as possible for\ntraining while at the same time ensuring that the validation (and possibly the\n:term:`testing`) set have sufficient size to provide numerically stable\nresults for the metrics used to quantify the model performance. The smaller\nthe validation set is, the larger the statistical uncertainty on the metrics\nwill be.\n\nUnder the hood, in :func:`spellbook.input.split_pddataframe_to_tfdatasets`,\nthe :class:`pandas.DataFrame` is split into two separate frames,\none for the features and one for the target labels. These are converted to\ntwo ``numpy.ndarray``\\s which are then used to initialise the\n:class:`tensorflow.data.Dataset` using the\n:func:`tensorflow.data.Dataset.from_tensor_slices` function. Finally, the\nsplit is applied:\n\n.. margin:: from :func:`spellbook.input.split_pddataframe_to_tfdatasets`\n\n    in :mod:`spellbook.input`\n\n.. code:: python\n\n    n = len(data)\n    n_train, n_val, n_test = calculate_splits(n, frac_train, frac_val,\n                                              n_train, n_val)\n\n    # separate features and labels\n    data_features = data[features]\n    data_labels = data[target]\n\n    # create a TensorFlow Dataset\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (data_features.values, data_labels.values))\n\n    # split it into train/val/test\n    train = dataset.take(n_train)\n    val = dataset.skip(n_train).take(n_val)\n    if n_test:\n        test = dataset.skip(n_train).skip(n_val).take(n_test)\n        return((train, val, test))\n    else:\n        return((train, val))\n\nNote that this does not shuffle and batch the resulting dataset. Shuffling\nmay be done\n\n- before in *pandas*: ``data = data.sample(frac = 1.0)``\n- or afterwards in *TensorFlow*:\n  ``train = train.shuffle(buffer_size = train.cardinality())``\n\nSince we shuffled the data before converting and splitting them, we can\nproceed to divide them into batches in *TensorFlow* with\n\n.. margin:: from **2-stroke-prediction-naive.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    train = train.batch(batch_size = 100)\n    val = val.batch(batch_size = 100)\n\nThe :term:`batch` size of ``100`` is chosen so that each batch contains at\nleast some stroke cases.\n\n\n\n### Option 2: Using *TensorFlow* Tensors\n\nUsed in ``3-stroke-prediction-oversampling.py``\nand implemented in :func:`spellbook.input.separate_tfdataset_to_tftensors`.\n\nJust like before, *TensorFlow* :class:`tf.data.Dataset`\\s for training and\nvalidation are created. This time, they are each split into two separate\n:class:`tf.Tensor`\\s - one for the features and one for the labels:\n\n.. margin:: from **3-stroke-prediction-oversampling.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    train, val = sb.input.split_pddataframe_to_tfdatasets(data, target, features, n_train=7000)\n    print(train.cardinality()) # print the size of the dataset\n    print(val.cardinality())\n    \n    # separate features and labels\n    train_features, train_labels = sb.input.separate_tfdataset_to_tftensors(train)\n    val_features, val_labels = sb.input.separate_tfdataset_to_tftensors(val)\n\nPlease don't mind the increased size of the training set for now - this script\nuses *oversampling* to deal with the imbalance in the ``stroke`` categories.\nWe will look into this in more detail later.\n    \nThe advantage of this :class:`tf.Tensor`-based approach is that it yields\na separate object containing just the target\nlabels, which can then be used when evaluating the model performance, e.g.\nwhen calculating a confusion matrix or a :term:`ROC` curve from comparisons\nof the predicted labels against the actual true target labels.\n\nInternally, :func:`spellbook.input.separate_tfdataset_to_tftensors` proceeds as follows:\n\n.. margin:: from :func:`spellbook.input.separate_tfdataset_to_tftensors`\n\n    in :mod:`spellbook.input`\n\n.. code:: python\n\n    # unpack the feature and label tensors from the dataset\n    # and reshape them into two separate tuples of tensors\n    features, labels = zip(*dataset)\n\n    # [...]\n\n    # which are then stacked to form the features and labels tensors\n    features = tf.stack(features)\n    labels = tf.stack(labels)\n\nFinally, since :class:`tf.Tensor`\\s cannot be batched, in this approach, the\nbatching is left to the call to :meth:`tf.keras.Model.fit` later after model\nsetup.\n\n\n\n\n### Option 3: Using *NumPy* Arrays\n\nUsed in ``4-stroke-prediction-oversampling-norm.py``\nand implemented in :func:`spellbook.input.split_pddataframe_to_nparrays`.\n\nThe third option is to split the dataset into :class:`numpy.ndarray`\\s:\n\n.. margin:: from **4-stroke-prediction-oversampling-norm.py**\n\n    in ``examples/1-binary-stroke-prediction/``\n\n.. code:: python\n\n    train_features, train_labels, val_features, val_labels \\\n        = sb.input.split_pddataframe_to_nparrays(\n            data, target, features, n_train=7000)\n\n    print(train_features.shape) # print the size of the dataset\n    print(train_labels.shape)\n    print(val_features.shape)\n    print(val_labels.shape)\n\nThe function :func:`spellbook.input.split_pddataframe_to_nparrays` works\nlike this in principle\n\n.. margin:: from :func:`spellbook.input.split_pddataframe_to_nparrays`\n\n    in :mod:`spellbook.input`\n\n.. code:: python\n\n    train = data.iloc[:n_train]\n    val = data.iloc[n_train:].iloc[:n_val]\n    result = [train[features].values, train[target].values,\n              val[features].values, val[target].values]\n    \n    # [...]\n    return tuple(result)\n\nLike with :class:`tf.Tensor`\\s in the previous approach, batching is left\nthe call to :meth:`tf.keras.Model.fit` later after model setup.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}