{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Serving *TensorFlow* Models in *Docker*\n\n.. raw:: html\n\n    <div class=\"tag-list\">\n       <div class=\"tag-cell tag-date\">June 21, 2021</div>\n       <div class=\"tag-cell tag-center\"></div>\n       <div class=\"tag-cell tag-right\">\n          <span class=\"tag-text\">tags:</span>\n          <a class=\"tag right\" href=\"../../search.html?q=tags+CNN\">\n             CNN</a>\n          <a class=\"tag right\" href=\"../../search.html?q=tags+Docker\">\n             Docker</a>\n          <a class=\"tag right\" href=\"../../search.html?q=tags+image+augmentation\">\n             image augmentation</a>\n          <a class=\"tag right\" href=\"../../search.html?q=tags+image+classification\">\n             image classification</a>\n          <a class=\"tag right\" href=\"../../search.html?q=tags+multi+class+classification\">\n             multi-class classification</a>\n        </div>\n    </div>\n\n\n.. admonition:: In this project/tutorial, we will\n   :class: spellbook-admonition-orange\n \n   - Train a **convolutional neural network** (**CNN**) to do\n     **multi-class classification** on Zalando's **Fashion-MNIST** dataset\n   - Use **image augmentation** to make the model more general\n   - Serve the model with *TensorFlow Serving* in a **Docker container**\n\nThe source code files for this tutorial are located in\n``examples/3-tensorflow-serving-docker/``.\n\nIn the first sections, we will have a look at the *Fashion-MNIST* dataset and\nset up and train a convolutional neural network. If you are just interested\nin the part about serving a model in *Docker*, please skip ahead to the\nfinal section `ex3-serving-docker`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The *Fashion-MNIST* Dataset\n\nZalando's [Fashion-MNIST](https://www.kaggle.com/zalando-research/fashionmnist) dataset is one of\nthe standard benchmarking datasets in computer vision, designed to be a\ndrop-in replacement for the original *MNIST* dataset of handwritten digits.\n*Fashion-MNIST* consists of greyscale images of different items of clothing.\nIt includes 60 000 images for training and 10 000 for validation and testing,\n28 pixels in height and 28 pixels in width, divided into 10 classes\nindicating the type of clothing:\n\n- **0**: t-shirt, top\n- **1**: trouser\n- **2**: pullover\n- **3**: dress\n- **4**: coat\n- **5**: sandal\n- **6**: shirt\n- **7**: sneaker\n- **8**: bag\n- **9**: ankle boot\n\n*TensorFlow* and *Keras* provide the *Fashion-MNIST* dataset as\n:class:`numpy.ndarray`\\s containing the 28x28 pixel values and the labels.\nThey can be loaded with\n\n.. margin:: from **1-plot.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), (val_images, val_labels) \\\n    = fashion_mnist.load_data()\n\nprint(type(train_images), type(train_labels))\nprint(train_images.shape, train_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each image is a 28x28 array of values between 0 and 255, with the background\nfilled with zeros and the object itself given with values larger than 0:\n\n.. margin:: from **1-plot.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nprint(np.array2string(train_images[0], max_line_width=150))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While it is possible to use just these arrays to train a neural network\nclassifier, let's go a step further and use\n:class:`tf.keras.preprocessing.image.ImageDataGenerator` for creating\na flow of images for training and validation.\nWith :class:`tf.keras.preprocessing.image.ImageDataGenerator` it is \npossible to flow images from :class:`numpy.ndarray`\\s, but also to load\nand flow them from directories or :class:`pandas.DataFrame`\\s containing\nthe image filepaths. At the same time,\n:class:`tf.keras.preprocessing.image.ImageDataGenerator` is able to\nprepare the labels, batch the images and the labels as well as apply\n:term:`image augmentation`. In image augmentation, transformations are\napplied to the images, including, but not limited to\n\n- horizontally or vertically flipping\n- rotating\n- zooming\n- shearing\n\nthem randomly within configurable ranges where applicable.\nThe randomness and the effective increase in the number of training\nimages reduce the likelihood of overtraining and the widened range of\npositions, orientations and appearances of the objects in the images\ncan help make the model more applicable to a larger number of images\nin inference.\nWe can set up the generator and obtain the flow iterator with\n\n.. margin:: from **helpers.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndef get_generator(images, labels, batch_size=32, **kwargs):\n\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        horizontal_flip=True,\n        rotation_range=20,\n        rescale=1/255)\n\n    gen = datagen.flow(\n        x = np.expand_dims(images, axis=3),\n        y = tf.keras.utils.to_categorical(labels, num_classes=10),\n        batch_size = batch_size,\n        **kwargs\n    )\n\n    return gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we also rescaled the images with a factor of 1/255 to restrict\nthe values to the range between 0 and 1. This is a customary input\nnormalisation since neural networks generally perform better with data\nof the order of one.\nWe also turned the simple integer class labels into one-hot encoded\nlabel vectors using :func:`tf.keras.utils.to_categorical`.\n\nWe can then instantiate a generator, e.g. for the training set, and\nretrieve the first batch of augmented images\n\n.. margin:: from **1-plot.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import helpers\n\nn = 7\n\ntrain_gen = helpers.get_generator(train_images, train_labels, batch_size=n,\n    shuffle=False)\n\ntrain_images_augmented = next(train_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we set ``shuffle=False`` because we want to preserve the order of\nthe images for comparing the augmented to the original images.\nSo let's proceed to plot a few images in two rows - the original images\nin the upper row and the corresponding augmented images in the lower\nrow. The result is shown in Figure 1.\n\n.. margin:: from **1-plot.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   import matplotlib as mpl\n   import matplotlib.pyplot as plt\n   import spellbook as sb\n\n   fig = plt.figure(figsize=(7,2))\n   grid = mpl.gridspec.GridSpec(nrows=2, ncols=n, wspace=0.1, hspace=0.1)\n\n   for i in range(n):\n       ax = plt.Subplot(fig, grid[0,i])\n       fig.add_subplot(ax)\n       ax.set_axis_off()\n       ax = plt.imshow(train_images[i], cmap='gray')\n\n       ax = plt.Subplot(fig, grid[1,i])\n       fig.add_subplot(ax)\n       ax.set_axis_off()\n       ax = plt.imshow(train_images_augmented[0][i], cmap='gray')\n\n   sb.plot.save(fig, 'images.png')\n\n.. figure:: /images/examples/3-tensorflow-serving-docker/images.png\n\n   Figure 1: The first few training images before and after augmentation\n\nAs we can see, the clothes are flipped and rotated as specified.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Neural Network Classifier\n\nWe are now going to set up a neural network for classifying the\n*Fashion-MNIST* images according to their respective labels.\n\n.. margin:: from **2-train.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters=30, kernel_size=(3,3), activation='relu',\n        input_shape=(28,28,1)),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=50, activation='relu'),\n    tf.keras.layers.Dense(units=10, activation='softmax')\n])\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The networks begins with a 2D convolutional layer (:term:`CNN`)\nwith 30 filters of 3x3 pixels each, followed by a max-pooling layer.\nSince each filter has 3x3=9 pixels and a bias, the total of 30 filters\ncorrespond to 300 trainable parameters. Sliding a 3x3 pixel filter across\n28x28 pixel images yields 26x26 pixel images and applying 2x2 pixel\nmax-pooling cuts the image size down to 13x13 pixels.\nThe flattening layer turns the 2D pixel arrays into a vector and\nfeeds them to the final part of the network, consisting of a dense\nlayer with 50 nodes and the output layer.\nSince we turned the labels into one-hot encoded label vectors, we\nuse a dense output layer with 10 nodes, i.e. one node per class,\nand :term:`softmax` activation. This ensures that the sum of all 10\noutputs of the last layer sum up to unity, which at least numerically\ncorresponds to properties expected from discreet probabilities. However,\nas long as a classifier is not calibrated, one cannot be sure that its\noutputs really give the probabilities for the different classes.\n\nThis network is deliberately simple because the main focus of this\ntutorial is on serving the model in *Docker* rather than achieving the\nbest possible performance. Improved performance can be achieved by\nadding more filters, more convolutional layers, followed by a larger\ndense network, while paying attention to overtraining and keeping it\nin check, e.g. by using :term:`dropout` layers.\n\nInstead of pursuing this approach and the correspondingly larger\ncomputational complexity, we will keep it fast and simple and proceed to\nconfigure the model with appropriate loss and metrics and finally train\nit for only just 3 epochs:\n\n.. margin:: from **2-train.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   model.compile(\n       loss = 'categorical_crossentropy',\n       optimizer = tf.keras.optimizers.Adam(),\n       metrics = [\n           tf.keras.metrics.CategoricalCrossentropy(),\n           tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n       ]\n   )\n\n   epochs = 3\n   history = model.fit(train_gen, epochs=epochs, validation_data=val_gen,\n       callbacks = [\n           tf.keras.callbacks.CSVLogger(filename='fmnist-model-history.csv'),\n           sb.train.ModelSavingCallback(foldername='fmnist-model')\n       ]\n   )\n\n.. code-output::\n\n   Epoch 1/3\n   938/938 [==============================] - 56s 59ms/step - loss: 0.5996 - categorical_crossentropy: 0.5996 - accuracy: 0.7853 - val_loss: 0.4732 - val_categorical_crossentropy: 0.4732 - val_accuracy: 0.8255\n   Epoch 2/3\n   938/938 [==============================] - 55s 58ms/step - loss: 0.4286 - categorical_crossentropy: 0.4286 - accuracy: 0.8444 - val_loss: 0.4286 - val_categorical_crossentropy: 0.4286 - val_accuracy: 0.8442\n   Epoch 3/3\n   938/938 [==============================] - 55s 58ms/step - loss: 0.3829 - categorical_crossentropy: 0.3829 - accuracy: 0.8601 - val_loss: 0.4052 - val_categorical_crossentropy: 0.4052 - val_accuracy: 0.8517\n   2021-06-20 13:17:43.055082: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n   Training finished after 3 epochs: Saved model to folder 'fmnist-model'</pre>\n\n``'categorical_crossentropy'`` sets an instance of\n:class:`tf.keras.losses.CategoricalCrossentropy` configured with the\ndefault values as the loss function. This is the appropriate loss function\nwhen using one-hot encoded labels. Lkewise, we are using the\n:class:`tf.keras.metrics.CategoricalCrossentropy` metric.\nFinally, the model is trained with ``model.fit``, passing instances\nof :class:`tf.keras.callbacks.CSVLogger` for saving the metrics to a\n``*.csv``-file during training and\n:class:`spellbook.train.ModelSavingCallback` for saving the model\nduring and at the end of the training. As we can see from the ouput, a\nclassification accuracy of about 85% is achieved during validation, which\ndoesn't seem too great, but is enough for our purposes in this tutorial.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating the Model\n\nBut before we go on, let's have just a slightly closer look at the\nmodel's performance. We begin by loading the saved model, preparing an\nimage generator for the validation dataset and use the model to calculate\nthe predictions:\n\n.. margin:: from **3-evaluate.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   model = tf.keras.models.load_model('fmnist-model')\n   val_gen = helpers.get_generator(val_images, val_labels, batch_size, shuffle=False)\n   val_predictions = model.predict(val_gen)\n   val_predicted_labels = np.argmax(val_predictions, axis=1)\n\nAgain we use ``shuffle=False`` so that the order of the predictions\ncorresponds to the order of the orginal labels in ``val_labels``.\n\nWe can then go on to determine and plot the confusion matrix with\n:func:`spellbook.plot.plot_confusion_matrix` - one version\nwith the absolute datapoint counts given in Figure 2 and one version\nnormalised across each true class in Figure 3:\n\n.. margin:: from **3-evaluate.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   class_ids = list(helpers.label_dict.keys())\n   class_names = list(helpers.label_dict.values())\n   val_confusion = tf.math.confusion_matrix(\n       val_labels, val_predicted_labels, num_classes=len(class_ids))\n\n   sb.plot.save(\n       sb.plot.plot_confusion_matrix(\n           confusion_matrix = val_confusion,\n           class_names = class_names,\n           class_ids = class_ids,\n           fontsize = 9.0,\n           fontsize_annotations = 'x-small'\n       ),\n       filename = 'fmnist-model-confusion.png'\n   )\n   sb.plot.save(\n       sb.plot.plot_confusion_matrix(\n           confusion_matrix = val_confusion,\n           class_names = class_names,\n           class_ids = class_ids,\n           normalisation = 'norm-true',\n           crop = False,\n           figsize = (6.4, 4.8),\n           fontsize = 9.0,\n           fontsize_annotations = 'x-small'\n       ),\n       filename = 'fmnist-model-confusion-norm-true.png'\n   )\n\n.. list-table::\n   :class: spellbook-gallery-wrap\n\n   * - .. figure:: /images/examples/3-tensorflow-serving-docker/fmnist-model-confusion.png\n          :height: 350px\n\n          Figure 2: Absolute datapoint counts\n\n     - .. figure:: /images/examples/3-tensorflow-serving-docker/fmnist-model-confusion-norm-true.png\n          :height: 350px\n\n          Figure 3: Relative frequencies normalised in each true category\n\nWe can see that by and large at least 75% of the items of each category are\ncorrectly classified, except for shirts which are most often confused with\nt-shirts/tops (15.8%) and to a lesser extent coats (8.7%) and pullovers\n(7.4%).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making Predictions About Other Pictures\n\nWhile evaluating and benchmarking a model's performance is still part of\nthe development process, the eventual interest is of course in deploying\nand using the model in production to obtain the predictions for images\nthat are not part of the training and validation sets.\nTo simulate this, I downloaded a few random images of different pieces\nof clothing and loaded them into :class:`numpy.ndarray`\\s using\n:func:`tf.keras.preprocessing.image.load_img` and\n:func:`tf.keras.preprocessing.image.img_to_array`.\n\n.. margin:: from **4-predict.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   import numpy as np\n   import tensorflow as tf\n\n   import helpers\n\n\n   model = tf.keras.models.load_model('fmnist-model')\n\n   test_images = helpers.load_images(helpers.tshirts)\n   # test_images = helpers.load_images(helpers.sandals)\n   # test_images = helpers.load_images(helpers.sneakers)\n\n\nWhen loading the images, it is important to size the arrays\nin accordance with the model's architecture and the images used during\ntraining and validation. Therefore, in this example, we choose a target\nsize of 28x28 pixels and use the ``'grayscale'`` colour mode:\n\n.. margin:: from **helpers.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   tshirts = [\n       'images/test/tshirt-1.jpg', 'images/test/tshirt-2.png',\n       'images/test/tshirt-3.jpg', 'images/test/tshirt-4.png'\n   ]\n   sandals = [f'images/test/sandal-{i}.jpg' for i in range(1, 5)]\n   sneakers = [f'images/test/sneaker-{i}.jpg' for i in range(1, 5)]\n\n\n   def load_images(images: Union[str, List[str]]):\n\n       if isinstance(images, str): images = [images]\n\n       array = np.empty(shape=(len(images), 28, 28, 1))\n       for i, image in enumerate(images):\n           img = tf.keras.preprocessing.image.load_img(\n               path = image,\n               color_mode = 'grayscale',\n               target_size = (28, 28))\n\n           array[i] = tf.keras.preprocessing.image.img_to_array(img=img)\n\n       return (255 - array) / 255\n\n\nOnce, the images are loaded, the ``model.predict`` function can be used\nto apply the model to the data:\n\n.. margin:: from **4-predict.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   predictions = model.predict(test_images)\n   for prediction in predictions:\n       print('prediction:', prediction,\n           '-> predicted class:', np.argmax(prediction))\n\n.. code-output::\n\n   prediction: [5.1984423e-01 4.8716479e-06 2.3578823e-04 4.8502727e-04 7.7355535e-06\n    1.2823233e-06 4.7876367e-01 1.8715612e-06 6.5485924e-04 6.3797620e-07] -> predicted class: 0\n   prediction: [2.1379247e-02 2.8888881e-04 5.3068418e-03 4.2415579e-04 2.2449503e-05\n    5.5147725e-04 5.9862167e-02 2.1699164e-07 9.1194308e-01 2.2140094e-04] -> predicted class: 8\n   prediction: [8.70128453e-01 1.32829140e-04 1.44031774e-02 2.06211829e-04\n    8.69949628e-03 4.22267794e-06 1.03566416e-01 5.67484494e-05\n    2.79841595e-03 4.09945960e-06] -> predicted class: 0\n   prediction: [9.6439028e-01 7.7955298e-07 1.6881671e-02 6.1920066e-03 2.3770966e-05\n    5.2960712e-07 1.2419004e-02 1.9606419e-09 9.1841714e-05 1.0729976e-09] -> predicted class: 0\n\nAs we can see, three of the four t-shirt images are correctly classified,\nwhile the second one is mistaken for a bag - which is perhaps as much as\ncan be expected with such a simple and only extremely briefly trained\nmodel.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Serving the Model in *Docker*\n\nFinally, let's see how we can serve the model inside\na *Docker* container using *TensorFlow Serving*.\nTo do that, first install *Docker* and get the ``tensorflow/serving``\nimage\n\n.. code:: bash\n\n   $ docker pull tensorflow/serving\n\nfrom [Docker Hub](https://hub.docker.com/r/tensorflow/serving).\nWhen started, a container created from this image will run\n``tensorflow_model_server`` and expose the REST API on port 8501\nThe model to serve can be specified via the ``MODEL_NAME`` and\n``MODEL_BASE_PATH`` environment variables.\nBy default, ``MODEL_BASE_PATH=/models`` and ``MODEL_NAME=model``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Custom Image for the Model\n\nTo serve our own model, we first need to create a custom *Docker* image\nbased on the ``tensorflow/serving`` image.\nFirst, create a container from the ``tensorflow/serving`` image and start it:\n\n.. code:: bash\n\n   $ sudo docker run -d --name tf-serving-base tensorflow/serving\n\n.. code-output::\n\n   4f9109df18bcace745d108dd1fba0659b65db62e5f4104f99ca4ed5536d194c6\n\nThis creates a container named ``tf-serving-base`` and returns the\ncontainer ID. We can verify that the container is running with\n\n.. code:: bash\n\n   $ sudo docker ps\n\n.. code-output::\n\n   CONTAINER ID   IMAGE                COMMAND                  CREATED         STATUS         PORTS           NAMES\n   4f9109df18bc   tensorflow/serving   \"/usr/bin/tf_serving\u2026\"   2 minutes ago   Up 2 minutes   8500-8501/tcp   tf-serving-base\n\nNext, we have to create a folder for the model inside the container.\nThis folder will later hold one subfolder for each version of the model\nso that the model can be seamlessly updated.\n\n.. code:: bash\n\n   $ sudo docker exec tf-serving-base mkdir /models/fmnist-model\n\nNow we can copy the saved model over to the container, creating the first\nversion subfolder at the same time:\n\n.. code:: bash\n\n   $ sudo docker cp fmnist-model tf-serving-base:/models/fmnist-model/1\n\nInstead of copying the model, we can also mount it into the container\nfrom the host filesystem when we start the container. While this will work\nwhen developing locally, it is of course not a good idea when the container\nis to be sent elsewhere.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If no version folder is created and the model is copied or mounted into\n   the model folder (``/models/fmnist-model/``) directly, the model cannot\n   be served and the following message will be shown:\n\n   .. code-output::\n\n      2021-06-17 19:17:06.689731: W tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:268] No versions of servable fmnist-model found under base path /models/fmnist-model. Did you forget to name your leaf directory as a number (eg. '/1/')?</p></div>\n\n\nOnce this is done, we can create a new image ``tf-serving-fmnist``\nfrom this container, specifying the name of the model folder as the\nenvironment variable ``MODEL_NAME``\n\n.. code:: bash\n\n   $ sudo docker commit --change \"ENV MODEL_NAME fmnist-model\" tf-serving-base tf-serving-fmnist\n\n.. code-output::\n\n   sha256:f34fefc2ee4ccc5d0b8a9ab756a48062fe23bcd4bc493b1fe165f66d7bfd3318\n\nand double-check the list of images\n\n.. code:: bash\n\n   $ sudo docker images\n\n.. code-output::\n\n   REPOSITORY           TAG       IMAGE ID       CREATED          SIZE\n   tf-serving-fmnist    latest    f34fefc2ee4c   57 seconds ago   411MB\n   tensorflow/serving   latest    e874bf5e4700   5 weeks ago      406MB\n\nFinally, we can stop the ``tf-serving-base`` container by doing\n\n- either\n\n  .. code:: bash\n\n     $ sudo docker stop tf-serving-base\n\n- or\n\n  .. code:: bash\n\n     $ sudo docker kill tf-serving-base\n\nwhich can readily be verified with ``sudo docker ps``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serving and Querying Our Own Model\n\nWe can create a container from the ``tf-serving-fmnist`` image and run it\nwith\n\n.. code:: bash\n\n   $ sudo docker run -p 8501:8501 -e MODEL_NAME=fmnist-model -t tf-serving-fmnist\n\nIn case we didn't copy the model into the container, we have to mount it\nfrom the host filesystem with\n\n.. code:: bash\n\n   $ sudo docker run -p 8501:8501 \\\n         --mount \\\n             type=bind,\\\n             source=/home/daniel/Computing/Programming/spellbook/examples/3-model-internal-image-preprocessing-pipeline/fmnist-model/,\\\n             target=/models/fmnist-model/1 \\\n         -e MODEL_NAME=fmnist-model \\\n         -t \\\n         tf-serving-fmnist\n\nWe can then query the model and obtain its predictions for some images by\nmeans of a ``POST`` request from the command line using\n``curl`` or from a python script using the ``requests`` module/library.\nThe example script ``5-request.py`` does both - it prints out a ``curl``\ncommand that can be copied, pasted and run in a terminal and it also submits\na request directly from the *Python* code and prints out the resulting\npredictions:\n\n.. margin:: from **5-request.py**\n\n   in ``examples/3-tensorflow-serving-docker/``\n\n.. code:: python\n\n   import json\n   import numpy as np\n   import requests\n\n   import helpers\n\n\n   test_images = helpers.load_images(helpers.tshirts)\n   # test_images = helpers.load_images(helpers.sandals)\n   # test_images = helpers.load_images(helpers.sneakers)\n\n   data = json.dumps({\n       'signature_name': 'serving_default',\n       'instances': test_images.tolist()   # *either* 'instances'\n       # 'inputs': test_images.tolist()    # *or* 'inputs'\n   })\n\n   print('------------------------------------------------------------')\n   print(\"for querying the served model from the terminal with 'curl',\"\n         \" use the following command\\n\")\n   print(\"curl -d '{}' -X POST {}\".format(\n       data, 'http://localhost:8501/v1/models/fmnist-model:predict'))\n   print('\\n------------------------------------------------------------')\n\n\n   headers = {'content-type': 'application/json'}\n   json_response = requests.post(\n       'http://localhost:8501/v1/models/fmnist-model:predict',\n       headers=headers,\n       data=data\n   )\n\n   print(json_response.text)\n   predictions = json.loads(json_response.text)['predictions'] # for 'instances'\n   # predictions = json.loads(json_response.text)['outputs']   # for 'inputs'\n   for i, prediction in enumerate(predictions):\n       print('prediction {}: {} -> predicted class: {}'.format(\n           i, prediction, np.argmax(prediction)))\n\nFor using a specific version of the model, e.g. version 2, the URL\n``http://localhost:8501/v1/models/fmnist-model/versions/2:predict``\ncan be used.\n\nOnce it has been created, the container can be stopped and started again\nwith\n\n.. code:: bash\n\n   $ sudo docker stop <CONTAINER-ID>\n\nand\n\n.. code:: bash\n\n   $ sudo docker start <CONTAINER-ID>\n\nIf a name was specified for the container with ``--name <NAME>``\nwhen creating it with the ``docker run`` command, then this name can be\nused to refer to the container instead of the ID.\n\n\n.. rubric:: Links\n\n- https://www.tensorflow.org/tfx/serving/docker\n- https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image\n- https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#make_a_request_to_your_model_in_tensorflow_serving\n- https://www.tensorflow.org/tfx/serving/api_rest#predict_api\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}