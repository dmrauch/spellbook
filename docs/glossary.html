
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Glossary &#8212; spellbook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/tooltip.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script defer="defer" src="_static/tooltip.js"></script>
    <script src="_static/glossary.json"></script>
    <script src="_static/design-tabs.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/spellbook-icon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Index" href="genindex.html" />
    <link rel="prev" title="Links &amp; Literature" href="literature.html" />
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/spellbook-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">spellbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search" aria-label="Search" autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The spellbook Library
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="API.html">
   Source Code Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="API-python.html">
     Python
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.input.html">
       spellbook.input
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.inspect.html">
       spellbook.inspect
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.plot.html">
       spellbook.plot
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.plot1D.html">
       spellbook.plot1D
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.plot2D.html">
       spellbook.plot2D
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.plotutils.html">
       spellbook.plotutils
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.stat.html">
       spellbook.stat
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="_stubs/spellbook.train.html">
       spellbook.train
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="tools.html">
   Tools and Technologies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/aws.html">
     AWS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/conda.html">
     Conda, Mamba &amp; Pip
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/docker.html">
     Docker
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/gcp.html">
     GCP
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="tools/git.html">
     Git
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="tools/github.html">
       GitHub
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/jupyter.html">
     Jupyter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/mkdocs.html">
     MkDocs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/pandas.html">
     pandas
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="tools/python.html">
     Python
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="tools/python-logging.html">
       Logging
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/rich.html">
     Rich
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/sphinx.html">
     Sphinx
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="tools/tf.html">
     TensorFlow
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="tools/tf-save.html">
       Saving and Loading Models
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/virtualenv.html">
     virtualenv &amp; venv
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tools/vscode.html">
     Visual Studio Code
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Projects &amp; Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="examples/examples.html">
   Table of Contents
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="examples/1-binary-stroke-prediction/index.html">
   Binary Classification: Stroke Prediction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="examples/1-binary-stroke-prediction/exploration-cleaning.html">
     Data Exploration and Cleaning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="examples/1-binary-stroke-prediction/input-pipeline.html">
     Data Preparation and Input Pipeline
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="examples/1-binary-stroke-prediction/training-validation.html">
     Model Training and Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="examples/2-tensorflow-decision-forests/index.html">
   Decision Forests in TensorFlow
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="_examples/3-tensorflow-serving-docker/code.html">
   Serving
   <em>
    TensorFlow
   </em>
   Models in
   <em>
    Docker
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="examples/4-tf-serving-docker-aws/index.html">
   <em>
    TensorFlow Serving
   </em>
   with
   <em>
    Docker
   </em>
   on
   <em>
    AWS
   </em>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Extras
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="literature.html">
   Links &amp; Literature
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="genindex.html">
   Index
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="todo.html">
   ToDo List
  </a>
 </li>
</ul>

    </div>
</nav><div class="navbar_extra_footer">
    <table style="margin-top:2.0rem;">
        <tr>
            <td style="text-align: left;">
                <a href="https://github.com/dmrauch">
                    <img src="_static/GitHub-Mark-32px.png" height="16" padding-right="1em">
                </a>
            </td>
            <td style="padding-left: 0.5rem; text-align: left; font-style:italic; font-weight: bold;">
                Contact me on <a href="https://github.com/dmrauch">GitHub</a>
            </td>
        </tr>
        <tr>
            <td style="text-align: left;">
                <a href="https://www.linkedin.com/in/dmrauch/">
                    <img src="_static/LI-In-Bug.png" height="16" padding-right="1em"></td>
                </a>
            <td style="padding-left: 0.5rem; text-align: left; font-style:italic; font-weight: bold;">
                Contact me on <a href="https://www.linkedin.com/in/dmrauch/">LinkedIn</a>
            </td>
        </tr>
    </table>
</div></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/dmrauch/spellbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/dmrauch/spellbook/issues/new?title=Issue%20on%20page%20%2Fglossary.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/dmrauch/spellbook/edit/master/doc/source/glossary.rst"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/glossary.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a">
   A
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#b">
   B
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c">
   C
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d">
   D
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#e">
   E
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#f">
   F
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#h">
   H
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i">
   I
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k">
   K
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l">
   L
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#m">
   M
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#o">
   O
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#p">
   P
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r">
   R
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#s">
   S
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t">
   T
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#u">
   U
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v">
   V
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Glossary</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a">
   A
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#b">
   B
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c">
   C
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#d">
   D
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#e">
   E
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#f">
   F
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#h">
   H
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#i">
   I
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k">
   K
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l">
   L
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#m">
   M
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#o">
   O
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#p">
   P
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r">
   R
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#s">
   S
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t">
   T
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#u">
   U
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#v">
   V
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="glossary">
<h1>Glossary<a class="headerlink" href="#glossary" title="Permalink to this headline">#</a></h1>
<section id="a">
<h2>A<a class="headerlink" href="#a" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-AUC">AUC<a class="headerlink" href="#term-AUC" title="Permalink to this term">#</a></dt><dd><p><em>Area Under the Curve</em></p>
<p>The area under a ROC curve. AUC is a metric that can be used to
benchmark different models against each other, with larger values
corresponding to better model performance.</p>
<p>See <a class="reference internal" href="#term-ROC"><span class="xref std std-term">ROC</span></a></p>
</dd>
</dl>
</section>
<section id="b">
<h2>B<a class="headerlink" href="#b" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-batch">batch<a class="headerlink" href="#term-batch" title="Permalink to this term">#</a></dt><dd><p>A batch is a subset of datapoints in a dataset that is processed
in sequence by a neural network before the loss function is evaluated,
the gradients are calculated and the model weights are updated
using backprop.</p>
<p>The larger the batch size is, the less frequently
these updates will happen and therefore, the less time each epoch will
take. On the flip side, the model parameters will not be updated
very often and the progress from epoch to the next may be slower.
Choosing smaller batch sizes on the other hand will lead to more
gradient evaluations, more frequent updates of the model weights and
therefore a longer duration of each epoch.</p>
</dd>
<dt id="term-bias">bias<a class="headerlink" href="#term-bias" title="Permalink to this term">#</a></dt><dd><p>The deviation of the predictions from the true labels in classification
and of the predicted values from the true values in regression</p>
</dd>
<dt id="term-binary-classification">binary classification<a class="headerlink" href="#term-binary-classification" title="Permalink to this term">#</a></dt><dd><p>Classification problem with only two classes. These are often
<em>positive</em> and <em>negative</em>, e.g. when a test for a disease is performed,
and the <em>negative</em> class is associated with the null hypothesis, i.e.
the hypothesis that is assumed to be valid until evidence of the
contrary is presented.</p>
<p>Contrast to <a class="reference internal" href="#term-multi-class-classification"><span class="xref std std-term">multi-class classification</span></a>.</p>
</dd>
</dl>
</section>
<section id="c">
<h2>C<a class="headerlink" href="#c" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-calibration">calibration<a class="headerlink" href="#term-calibration" title="Permalink to this term">#</a></dt><dd><p><em>Calibration</em> refers to the process of checking and correcting
the <a class="reference internal" href="#term-score"><span class="xref std std-term">score</span></a> of a model in terms of its possible interpretation
as a probability. For example, in <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a>, the
<a class="reference internal" href="#term-sigmoid"><span class="xref std std-term">sigmoid</span></a>-activated model output is often interpreted as the
probability that the datapoint belongs to the <em>positive</em> class.
However, the fact that the <a class="reference internal" href="#term-sigmoid"><span class="xref std std-term">sigmoid</span></a> function returns values
in [0, 1] does not yet guarantee that its values accurately quantify
the probability for a datapoint to belong to a certain class. These
probabilities and their relation to the model <a class="reference internal" href="#term-score"><span class="xref std std-term">score</span></a> are
determined during <em>calibration</em>.</p>
<p>The principle roughly is the following, illustrated with the example
of <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a> in mind:</p>
<ul class="simple">
<li><p>Choose a certain threshold for the model <a class="reference internal" href="#term-score"><span class="xref std std-term">score</span></a>, above which
a datapoint is sorted into the positive category</p></li>
<li><p>Apply this threshold and</p>
<ul>
<li><p>average the predicted scores over all datapoints that exceed the
threshold</p></li>
<li><p>among the datapoints exceeding the threshold, determine the
fraction that truly belong to the <em>positive</em> class</p></li>
</ul>
</li>
<li><p>Add a point to the calibration plot with the average predicted score
on the x-axis and the fraction of true positives on the y-axis</p></li>
<li><p>Repeat this with a number of different thresholds</p></li>
</ul>
<p>This results in a <em>calibration curve</em>, e.g. such as the one in
<a class="reference external" href="https://scikit-learn.org/stable/modules/calibration.html#calibration-curves">https://scikit-learn.org/stable/modules/calibration.html#calibration-curves</a>.</p>
</dd>
<dt id="term-CL">CL<a class="headerlink" href="#term-CL" title="Permalink to this term">#</a></dt><dd><p><a class="reference internal" href="#term-confidence-level"><span class="xref std std-term">confidence level</span></a></p>
</dd>
<dt id="term-CNN">CNN<a class="headerlink" href="#term-CNN" title="Permalink to this term">#</a></dt><dd><p><em>Convolutional Neural Network</em></p>
<p>A neural network containing one or more convolutional layers.</p>
<p>In a 2D convolutional layer, typically used for images, <span class="math notranslate nohighlight">\(n_f\)</span>
two-dimensional filters of size <span class="math notranslate nohighlight">\(f_1 \times f_2\)</span> are slid
across the two-dimensional data arrays of size <span class="math notranslate nohighlight">\(n_1 \times n_2\)</span>
to create <span class="math notranslate nohighlight">\(n_f\)</span> output data arrays of size
<span class="math notranslate nohighlight">\((n_1-f_1+1) \times (n_2-f_2+1)\)</span>. Analogous convolutional
layers of different dimensionalities exist as well.
Convolutional layers are often followed by pooling layers that
aggregate neighbouring pixels or voxels by calculating their maximum
or average.</p>
<p>By training and adjusting the filters, the neural network can
distill particular patterns in the data and feed them to the
following dense layers.</p>
</dd>
<dt id="term-confidence-level">confidence level<a class="headerlink" href="#term-confidence-level" title="Permalink to this term">#</a></dt><dd><p>The probability of <em>not</em> making a
<a class="reference internal" href="#term-type-1-error"><span class="xref std std-term">type-1 error</span></a>, i.e. the probability of <em>not</em> wrongly rejecting
the null hypothesis and therefore rightly accepting the null hypothesis.</p>
<div class="math notranslate nohighlight">
\[\text{confidence level} = \text{CL} = 1 - \alpha\]</div>
</dd>
</dl>
</section>
<section id="d">
<h2>D<a class="headerlink" href="#d" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-data-augmentation">data augmentation<a class="headerlink" href="#term-data-augmentation" title="Permalink to this term">#</a></dt><dd><p>Techniques for artificially increasing the size of the dataset</p>
<p>For example, in computer vision, images in the training set
may be subjected to random shifts, rotations, shearing, horizontal
flipping, changes in brightness, contrast, saturation and other
properties.</p>
<p>This can help to increase the model performance by allowing for
more and longer training while at the same time avoiding
<a class="reference internal" href="#term-overtraining"><span class="xref std std-term">overtraining</span></a>.</p>
</dd>
<dt id="term-dropout">dropout<a class="headerlink" href="#term-dropout" title="Permalink to this term">#</a></dt><dd><p>A <em>dropout layer</em> in a neural network randomly sets some of the values
passed into it from the preceding layer to zero, i.e. randomly drops or
deactivates some of its inputs. The fraction of dropped nodes, usually
called <em>dropout rate</em>, is a model <a class="reference internal" href="#term-hyperparameter"><span class="xref std std-term">hyperparameter</span></a>.</p>
<p>The original paper: G.E. Hinton et al: <em>Improving neural networks by
preventing co-adaptation of feature detectors</em>, <a class="reference external" href="https://arxiv.org/abs/1207.0580">arXiv:1207.0580</a></p>
</dd>
</dl>
</section>
<section id="e">
<h2>E<a class="headerlink" href="#e" title="Permalink to this headline">#</a></h2>
<dl class="glossary simple">
<dt id="term-EBS">EBS<a class="headerlink" href="#term-EBS" title="Permalink to this term">#</a></dt><dd><p>Amazon <em>Elastic Block Store</em></p>
</dd>
<dt id="term-EC2">EC2<a class="headerlink" href="#term-EC2" title="Permalink to this term">#</a></dt><dd><p>Amazon <em>Elastic Cloud Compute</em>: Virtual servers</p>
</dd>
<dt id="term-ECR">ECR<a class="headerlink" href="#term-ECR" title="Permalink to this term">#</a></dt><dd><p>Amazon <em>Elastic Container Registry</em>: Repositories for <em>Docker</em>
containers</p>
</dd>
</dl>
</section>
<section id="f">
<h2>F<a class="headerlink" href="#f" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-FN">FN<a class="headerlink" href="#term-FN" title="Permalink to this term">#</a></dt><dd><p><em>False Negative</em>: The label of a datapoint is predicted to be
<em>negative</em>, but is <em>positive</em> in reality.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>False negatives can be particularly dangerous as e.g. a
patient who really has a condition is not detected as sick and
therefore is not treated.</p>
</div>
</dd>
<dt id="term-FP">FP<a class="headerlink" href="#term-FP" title="Permalink to this term">#</a></dt><dd><p><em>False Positive</em>: The label of a datapoint is predicted to be
<em>positive</em>, but is <em>negative</em> in reality.</p>
</dd>
<dt id="term-FPR">FPR<a class="headerlink" href="#term-FPR" title="Permalink to this term">#</a></dt><dd><p><em>False Positive Rate</em></p>
<p>Defined as</p>
<div class="math notranslate nohighlight">
\[\text{FPR} := \frac{\text{FP}}{\text{TN} + \text{FP}}
= 1 - \text{specificity} \approx \alpha\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{FP}\)</span> are the false positives and <span class="math notranslate nohighlight">\(\text{TN}\)</span>
the true negatives.</p>
<p>It specifies what fraction of the truly negative datapoints were
incorrectly classified / predicted to be positive. Therefore, it is
related to the <a class="reference internal" href="#term-type-1-error"><span class="xref std std-term">type-1 error</span></a> and its probability <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</dd>
</dl>
</section>
<section id="h">
<h2>H<a class="headerlink" href="#h" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-hyperparameter">hyperparameter<a class="headerlink" href="#term-hyperparameter" title="Permalink to this term">#</a></dt><dd><p><em>Hyperparameters</em> characterise the layout and architecture of the
model and its associated functions and algorithms. As such,
<em>hyperparameters</em> are not and cannot be changed
during training. There are two different types of <em>hyperparameters</em>:</p>
<ul class="simple">
<li><p><strong>model hyperparameters</strong>: e.g. the number of hidden layers in a
neural network, the <a class="reference internal" href="#term-dropout"><span class="xref std std-term">dropout</span></a> rate, the activation function
of a specific layer</p></li>
<li><p><strong>algorithm hyperparameters</strong>: e.g. the optimiser, its learning
rate, the batch size</p></li>
</ul>
<p><em>Hyperparameters</em> can be searched and optimised to maximise model
performance. This process is called <a class="reference internal" href="#term-hyperparameter-tuning"><span class="xref std std-term">hyperparameter tuning</span></a> or
<a class="reference internal" href="#term-hyperparameter-optimisation"><span class="xref std std-term">hyperparameter optimisation</span></a>.</p>
<p>Contrast against <a class="reference internal" href="#term-model-parameter"><span class="xref std std-term">model parameter</span></a>.</p>
</dd>
<dt id="term-hyperparameter-optimisation">hyperparameter optimisation<a class="headerlink" href="#term-hyperparameter-optimisation" title="Permalink to this term">#</a></dt><dd><p>see <a class="reference internal" href="#term-hyperparameter-tuning"><span class="xref std std-term">hyperparameter tuning</span></a></p>
</dd>
<dt id="term-hyperparameter-tuning">hyperparameter tuning<a class="headerlink" href="#term-hyperparameter-tuning" title="Permalink to this term">#</a></dt><dd><p>Evaluation of the achievable model performance when
trying out different values for one or more hyperparameters. Normally,
<em>hyperparameter tuning</em> refers to automated strategies for scanning
different <a class="reference internal" href="#term-hyperparameter"><span class="xref std std-term">hyperparameter</span></a> values and ranges.</p>
<p>Since evaluating a single point in hyperparameter space involves
training and validating a model, <em>hyperparameter tuning</em> can be quite
time-consuming and resource-intensive. Therefore, normally, not the
full hyperparameter space is scanned for a model, but rather a
subset.</p>
<p>Tuning strategies broadly fall into three basic categories:</p>
<ul class="simple">
<li><p><strong>grid searches</strong>: All possible combinations of the selected
hyperparameters and their values are tried out systematically.
For categorical hyperparameters, e.g. the choice of the optimiser,
all specified options are tried, and for continuous and ordinal
hyperparameters, linearly or logarithmically equidistant points
within configured ranges may be tried.</p></li>
<li><p><strong>random searches</strong>: Points in the configured hyperparameter space
are picked randomly</p></li>
<li><p><strong>advanced searches</strong>: Advanced searches try to make informed
decisions on which hyperparameter point to evaluate next, based
on which hyperparameter points were scanned before and how they
performed. A typical strategy is Bayesian optimisation together with
Gaussian random processes.</p></li>
</ul>
</dd>
</dl>
</section>
<section id="i">
<h2>I<a class="headerlink" href="#i" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-IAM">IAM<a class="headerlink" href="#term-IAM" title="Permalink to this term">#</a></dt><dd><p>AWS <em>Identity &amp; Access Management</em></p>
</dd>
<dt id="term-image-augmentation">image augmentation<a class="headerlink" href="#term-image-augmentation" title="Permalink to this term">#</a></dt><dd><p>In <em>image augmentation</em>, transformations are applied to images
before feeding them into a model. These transformations can serve
to normalise the images, e.g. by rescaling them with a common
factor, as well as to effectively increase the size of the datasets by
applying random flips, rotations, brightness changes and other
transformations. While these random transformations can help protect
against <a class="reference internal" href="#term-overtraining"><span class="xref std std-term">overtraining</span></a>, they can also help the trained model
in generalising to other images. For example, this would be the
case with the <em>Fashion-MNIST</em> dataset which, among other types of
clothes, contains shoes which are all pointing with their tips to
the left.</p>
</dd>
<dt id="term-imbalanced-data">imbalanced data<a class="headerlink" href="#term-imbalanced-data" title="Permalink to this term">#</a></dt><dd><p>When the data contain significantly more datapoints in one class than
the other(s), in <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a> or
<a class="reference internal" href="#term-multi-class-classification"><span class="xref std std-term">multi-class classification</span></a>.</p>
<p>See <a class="reference internal" href="examples/1-binary-stroke-prediction/index.html"><span class="doc">Binary Classification with the Stroke Prediction Dataset</span></a></p>
</dd>
</dl>
</section>
<section id="k">
<h2>K<a class="headerlink" href="#k" title="Permalink to this headline">#</a></h2>
<dl class="glossary simple">
<dt id="term-KMS">KMS<a class="headerlink" href="#term-KMS" title="Permalink to this term">#</a></dt><dd><p>AWS <em>Key Management Service</em></p>
</dd>
</dl>
</section>
<section id="l">
<h2>L<a class="headerlink" href="#l" title="Permalink to this headline">#</a></h2>
<dl class="glossary simple">
<dt id="term-L1-regularisation">L1 regularisation<a class="headerlink" href="#term-L1-regularisation" title="Permalink to this term">#</a></dt><dd><p>When <em>L1</em> (or <em>lasso</em>) <em>regularisation</em> is activated for a layer, a
penalty term <em>proportional to the sum of the absolute values</em> of the
weights of that layer is added to the loss function. The strength
of the regularisation can be adjusted by scaling the penalty term
with a factor.</p>
</dd>
<dt id="term-L2-regularisation">L2 regularisation<a class="headerlink" href="#term-L2-regularisation" title="Permalink to this term">#</a></dt><dd><p>When <em>L2</em> (or <em>ridge</em>) <em>regularisation</em> is activated for a layer, a
penalty term <em>quadratic in the sum</em> of the weights of that layer is
added to the loss function. The strength of the regularisation can
be adjusted by scaling the penalty term with a factor.</p>
</dd>
<dt id="term-lasso-regularisation">lasso regularisation<a class="headerlink" href="#term-lasso-regularisation" title="Permalink to this term">#</a></dt><dd><p>see <a class="reference internal" href="#term-L1-regularisation"><span class="xref std std-term">L1 regularisation</span></a></p>
</dd>
</dl>
</section>
<section id="m">
<h2>M<a class="headerlink" href="#m" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-model-parameter">model parameter<a class="headerlink" href="#term-model-parameter" title="Permalink to this term">#</a></dt><dd><p><em>Model parameters</em> are the parameters adjusted during training to
minimise the loss function and fit the model to the training data,
e.g. the weights of the edges between the nodes in a neural network.</p>
<p>Contrast against <a class="reference internal" href="#term-hyperparameter"><span class="xref std std-term">hyperparameter</span></a>.</p>
</dd>
<dt id="term-multi-class-classification">multi-class classification<a class="headerlink" href="#term-multi-class-classification" title="Permalink to this term">#</a></dt><dd><p>Classification problem involving more than two classes</p>
<p>Contrast to <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a>.</p>
</dd>
</dl>
</section>
<section id="o">
<h2>O<a class="headerlink" href="#o" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-overfitting">overfitting<a class="headerlink" href="#term-overfitting" title="Permalink to this term">#</a></dt><dd><p>see <a class="reference internal" href="#term-overtraining"><span class="xref std std-term">overtraining</span></a></p>
</dd>
<dt id="term-oversampling">oversampling<a class="headerlink" href="#term-oversampling" title="Permalink to this term">#</a></dt><dd><p>Method for addressing <a class="reference internal" href="#term-imbalanced-data"><span class="xref std std-term">imbalanced data</span></a></p>
<p>See <a class="reference internal" href="examples/1-binary-stroke-prediction/index.html"><span class="doc">Binary Classification with the Stroke Prediction Dataset</span></a></p>
</dd>
<dt id="term-overtraining">overtraining<a class="headerlink" href="#term-overtraining" title="Permalink to this term">#</a></dt><dd><p>Also called <em>overfitting</em></p>
<p>When the model memorises specific random fluctuations in the training
data. Since the validation does not contain the exact same datapoints,
but rather others with different random fluctuations, the model fails
to generalise to the validation data. Therefore, when <em>overtraining</em>
occurs, the model performance is worse during validation than in
training.</p>
<p>In training, the <em>predicted</em> values lie close to the <em>true</em> values,
but the model fails to generalise beyond the specific datapoints,
corresponding to a low <a class="reference internal" href="#term-bias"><span class="xref std std-term">bias</span></a> but high <a class="reference internal" href="#term-variance"><span class="xref std std-term">variance</span></a>.</p>
<p><em>Overtraining</em> may occur when</p>
<ul class="simple">
<li><p>the model is too complex, i.e. it has too many parameters</p></li>
<li><p>training continues for too long on a too limited dataset</p></li>
</ul>
<p>There are several strategies aimed at avoiding <em>overtraining</em>:</p>
<ul class="simple">
<li><p>more training data</p></li>
<li><p>early stopping of the training, when the loss and accuracy do not
improve anymore</p></li>
<li><p>a less complex model with fewer parameters</p></li>
<li><p>regularisation techniques</p>
<ul>
<li><p><a class="reference internal" href="#term-dropout"><span class="xref std std-term">dropout</span></a> layers</p></li>
<li><p><a class="reference internal" href="#term-L1-regularisation"><span class="xref std std-term">L1 regularisation</span></a> or <a class="reference internal" href="#term-L2-regularisation"><span class="xref std std-term">L2 regularisation</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#term-data-augmentation"><span class="xref std std-term">data augmentation</span></a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="p">
<h2>P<a class="headerlink" href="#p" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-power">power<a class="headerlink" href="#term-power" title="Permalink to this term">#</a></dt><dd><p>The <em>power</em> of a test or classifier quantifies its capability of
detecting a <em>positive</em> result. Therefore, it is related to the
probability of the <a class="reference internal" href="#term-type-2-error"><span class="xref std std-term">type-2 error</span></a> <span class="math notranslate nohighlight">\(\beta\)</span> by:</p>
<div class="math notranslate nohighlight">
\[\text{power} = 1 - \beta\]</div>
<p>See also: <a class="reference internal" href="#term-TPR"><span class="xref std std-term">TPR</span></a></p>
</dd>
<dt id="term-precision">precision<a class="headerlink" href="#term-precision" title="Permalink to this term">#</a></dt><dd><p>Defined as</p>
<div class="math notranslate nohighlight">
\[\text{precision} := \frac{\text{TP}}{\text{TP} + \text{FP}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{TP}\)</span> are the true positives and <span class="math notranslate nohighlight">\(\text{FP}\)</span>
the false positives.</p>
<p>It specifies what fraction of the datapoints that were
classified/predicted to be <em>positive</em> are in fact truly <em>positive</em>,
i.e. which fraction of the <em>positive</em> classifications/predictions
is correct. Therefore, e.g. in the context of medical tests,
the <em>precision</em> is of special interest to the tested person or
patient because it gives the probability for the <em>positive</em> result
to be actually true.</p>
</dd>
</dl>
</section>
<section id="r">
<h2>R<a class="headerlink" href="#r" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-recall">recall<a class="headerlink" href="#term-recall" title="Permalink to this term">#</a></dt><dd><p>see <em>True Positive Rate</em> (<a class="reference internal" href="#term-TPR"><span class="xref std std-term">TPR</span></a>)</p>
</dd>
<dt id="term-ridge-regularisation">ridge regularisation<a class="headerlink" href="#term-ridge-regularisation" title="Permalink to this term">#</a></dt><dd><p>see <a class="reference internal" href="#term-L2-regularisation"><span class="xref std std-term">L2 regularisation</span></a></p>
</dd>
<dt id="term-ROC">ROC<a class="headerlink" href="#term-ROC" title="Permalink to this term">#</a></dt><dd><p><em>Receiver Operator Characteristic</em></p>
<p>The ROC curve shows the <em>true positive rate</em> (<a class="reference internal" href="#term-TPR"><span class="xref std std-term">TPR</span></a>) vs. the
<em>false positive rate</em> (<a class="reference internal" href="#term-FPR"><span class="xref std std-term">FPR</span></a>) for a given model. So it
essentially gives the balance between type-1 and type-2 errors and
visualises to what extent decreasing one will increase the other.
Choosing a certain threshold value of the activated classifier
output (and thereby defining the rule for associating datapoints with
classes) corresponds to picking a working point somewhere on a given
ROC curve and moving the threshold value scans the ROC curve so that
a working point with the desired balance of error rates can be picked.</p>
<p>The more the ROC curve extends to the top left corner, i.e. towards
high TPRs at low FPRs, the better the performance of a model.
Therefore, the <em>area under the curve</em> (<a class="reference internal" href="#term-AUC"><span class="xref std std-term">AUC</span></a>) of a ROC curve can
be used to benchmark different models against each other.</p>
</dd>
</dl>
</section>
<section id="s">
<h2>S<a class="headerlink" href="#s" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-sample">sample<a class="headerlink" href="#term-sample" title="Permalink to this term">#</a></dt><dd><p>In datascience, <em>sample</em> refers to a single datapoint.</p>
<p>Since I have a background in particle physics, where the term <em>sample</em>
usually refers to a set of generated/simulated datapoints, I tend to
avoid it and usually prefer <em>datapoint</em>.</p>
<p>The vocabulary ‘confusion matrix’ that translates between data science
and particle physics is the following:</p>
<table class="table">
<colgroup>
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 64%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>object</p></th>
<th class="head"><p>data science</p></th>
<th class="head"><p>particle physics</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single entity</p></td>
<td><p>sample</p></td>
<td><p>datapoint</p></td>
</tr>
<tr class="row-odd"><td><p>set of entities</p></td>
<td><p>dataset</p></td>
<td><ul class="simple">
<li><p>if <em>measured</em>: dataset</p></li>
<li><p>if <em>generated/simulated</em>: (Monte Carlo) sample</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
<dt id="term-score">score<a class="headerlink" href="#term-score" title="Permalink to this term">#</a></dt><dd><p>The <em>score of a model</em> is the activated output of a model, e.g. the
activated output of the last layer in a neural network.</p>
<p>The unactivated outputs are called <em>logits</em>.</p>
<p>Commonly used activation functions are</p>
<ul class="simple">
<li><p><a class="reference internal" href="#term-sigmoid"><span class="xref std std-term">sigmoid</span></a> activation in <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a></p></li>
<li><p><a class="reference internal" href="#term-softmax"><span class="xref std std-term">softmax</span></a> activation in <a class="reference internal" href="#term-multi-class-classification"><span class="xref std std-term">multi-class classification</span></a></p></li>
</ul>
</dd>
<dt id="term-sensitivity">sensitivity<a class="headerlink" href="#term-sensitivity" title="Permalink to this term">#</a></dt><dd><p>see <em>True Positive Rate</em> (<a class="reference internal" href="#term-TPR"><span class="xref std std-term">TPR</span></a>)</p>
</dd>
<dt id="term-sigmoid">sigmoid<a class="headerlink" href="#term-sigmoid" title="Permalink to this term">#</a></dt><dd><p>Sigmoid functions follow a characteristic ‘S’-shape. In machine
learning, <em>sigmoid activation</em> usually refers to using the
<em>logistic function</em></p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{1 + e^{-x}}\]</div>
<p>as the activation function.</p>
<p>Since the <em>sigmoid function</em> maps all real numbers to
the interval (0, 1), <em>sigmoid activation</em> is typically used in
<a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a>, with outputs close to 0 associated to
one category and outputs close to 1 to the other. The sigmoid-
activated network output is also often interpreted as the probability
of a datapoint to belong to the second class, but this interpretation
has to be taken with a grain of salt, see <a class="reference internal" href="#term-calibration"><span class="xref std std-term">calibration</span></a>.</p>
</dd>
<dt id="term-softmax">softmax<a class="headerlink" href="#term-softmax" title="Permalink to this term">#</a></dt><dd><p>The <em>softmax</em> function is typically used as the activation function
in <a class="reference internal" href="#term-multi-class-classification"><span class="xref std std-term">multi-class classification</span></a> problems with one-hot
encoded labels. It is defined as</p>
<div class="math notranslate nohighlight">
\[\sigma(\vec{x})_i = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\]</div>
<p>Each of the <span class="math notranslate nohighlight">\(n\)</span> target classes corresponds to one entry in
the classification vector <span class="math notranslate nohighlight">\(\vec{x}\)</span> and the <em>softmax</em> function
provides a mapping <span class="math notranslate nohighlight">\(\mathbb{R}^n \to [0,1]^n\)</span>. Furthermore,
it provides a normalisation such that the activated entries of the
classification vector sum up to unity, i.e.
<span class="math notranslate nohighlight">\(\sum_{i=1}^n \sigma(\vec{x})_i = 1\)</span>.
This is what is naturally expected for discreet probabilities.
However, as long as a classifier is not calibrated, it cannot be
guaranteed that the activated output of the last layer gives the
probabilities for a datapoint to belong to each of the involved
target classes.</p>
</dd>
<dt id="term-specificity">specificity<a class="headerlink" href="#term-specificity" title="Permalink to this term">#</a></dt><dd><p>Defined as</p>
<div class="math notranslate nohighlight">
\[\text{specificity} := \frac{\text{TN}}{\text{TN} + \text{FP}}
= 1 - \text{FPR}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{TN}\)</span> are the true negatives and <span class="math notranslate nohighlight">\(\text{FP}\)</span>
the false positives.</p>
<p>It specifies what fraction of the truly <em>negative</em> datapoints was
correctly classified/predicted to be <em>negative</em>. Therefore, the
<em>specificity</em> is related to the <a class="reference internal" href="#term-FPR"><span class="xref std std-term">FPR</span></a>.</p>
</dd>
</dl>
</section>
<section id="t">
<h2>T<a class="headerlink" href="#t" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-testing">testing<a class="headerlink" href="#term-testing" title="Permalink to this term">#</a></dt><dd><p>The determination of the <em>unbiased</em> model performance.</p>
<p>To this end, the full dataset available during model design, training
and development is split up into three distinct parts:</p>
<ul class="simple">
<li><p>the <em>training</em> dataset</p></li>
<li><p>the <em>validation</em> / <em>hold-out cross-validation</em> or
<em>development</em> dataset and</p></li>
<li><p>the <em>test</em> dataset</p></li>
</ul>
<p>While the model parameters are adjusted on the <em>training</em> dataset, the
performance of the model during the development phase is estimated from
the <em>validation</em> dataset. Between the training runs, the
hyperparameters are changed so as to maximise the performance metrics
evaluated from the <em>validation</em> dataset. Finally, at the end of the
development phase, a specific model and a set of hyperparameters is
chosen and afterwards, the model performance is evaluated based on the
<em>test</em> dataset. This is an unbiased estimate since the <em>test</em> data
were never previously used to make choices regarding the model.</p>
<p>Many times, when getting a proper unbiased estimate of the model
performance is not crucial, no separate testing is performed. In such
cases, the model performance is simply quantified with the validation
results. In practice, this validation stage is then often referred to
as ‘testing’.</p>
</dd>
<dt id="term-TN">TN<a class="headerlink" href="#term-TN" title="Permalink to this term">#</a></dt><dd><p><em>True Negative</em>: The label of a datapoint is predicted to be
<em>negative</em> and also is <em>positive</em> in reality</p>
</dd>
<dt id="term-TP">TP<a class="headerlink" href="#term-TP" title="Permalink to this term">#</a></dt><dd><p><em>True Positive</em>: The label of a datapoint is predicted to be
<em>positive</em> and also is <em>positive</em> in reality</p>
</dd>
<dt id="term-TPR">TPR<a class="headerlink" href="#term-TPR" title="Permalink to this term">#</a></dt><dd><p><em>True Positive Rate</em></p>
<p>Defined as</p>
<div class="math notranslate nohighlight">
\[\text{TPR} := \frac{\text{TP}}{\text{TP} + \text{FN}}
= \text{sensitivity}
= \text{recall} \approx 1 - \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{TP}\)</span> are the true positives and <span class="math notranslate nohighlight">\(\text{FN}\)</span>
the false negatives.</p>
<p>It specifies what fraction of the truly positive datapoints were
correctly classified / predicted to be positive. Therefore, it is
related to the <a class="reference internal" href="#term-type-2-error"><span class="xref std std-term">type-2 error</span></a> and its probability <span class="math notranslate nohighlight">\(\beta\)</span>,
or, more specifically the <a class="reference internal" href="#term-power"><span class="xref std std-term">power</span></a> <span class="math notranslate nohighlight">\(1 - \beta\)</span>.</p>
</dd>
<dt id="term-type-1-error">type-1 error<a class="headerlink" href="#term-type-1-error" title="Permalink to this term">#</a></dt><dd><p>The error of wrongly rejecting the null hypothesis and accepting the
alternative hypothesis. Its probability is denoted with <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[\alpha := P(\text{type-1 error})\]</div>
<p>It is related to the <a class="reference internal" href="#term-confidence-level"><span class="xref std std-term">confidence level</span></a> (CL) by</p>
<div class="math notranslate nohighlight">
\[\alpha = 1 - \text{confidence level}\]</div>
<p>In <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a>, where the null hypothesis is usually
taken to be</p>
<ul class="simple">
<li><p>a negative test</p></li>
<li><p>the patient is healthy</p></li>
<li><p>the absence of new physics effects and the validity of the currently
established model</p></li>
</ul>
<p>or a similarly <em>normal</em> situation, making type-1 errors results in
<em>false positives</em> (<a class="reference internal" href="#term-FP"><span class="xref std std-term">FP</span></a>).</p>
</dd>
<dt id="term-type-2-error">type-2 error<a class="headerlink" href="#term-type-2-error" title="Permalink to this term">#</a></dt><dd><p>The error of wrongly accepting the null hypothesis and rejecting the
alternative hypothesis. Its probability is denoted with <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\beta := P(\text{type-2 error})\]</div>
<p>It is related to the <a class="reference internal" href="#term-power"><span class="xref std std-term">power</span></a> by</p>
<div class="math notranslate nohighlight">
\[\beta = 1 - \text{power}\]</div>
<p>In <a class="reference internal" href="#term-binary-classification"><span class="xref std std-term">binary classification</span></a>, where the null hypothesis is usually
taken to be</p>
<ul class="simple">
<li><p>a negative test</p></li>
<li><p>the patient is healthy</p></li>
<li><p>the absence of new physics effects and the validity of the currently
established model</p></li>
</ul>
<p>or a similarly <em>normal</em> situation, making type-2 errors results in
<em>false negatives</em> (<a class="reference internal" href="#term-FN"><span class="xref std std-term">FN</span></a>).</p>
</dd>
</dl>
</section>
<section id="u">
<h2>U<a class="headerlink" href="#u" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-underfitting">underfitting<a class="headerlink" href="#term-underfitting" title="Permalink to this term">#</a></dt><dd><p>see <a class="reference internal" href="#term-undertraining"><span class="xref std std-term">undertraining</span></a></p>
</dd>
<dt id="term-undertraining">undertraining<a class="headerlink" href="#term-undertraining" title="Permalink to this term">#</a></dt><dd><p>Also called <em>underfitting</em>.</p>
<p>When the model fails to learn the characteristic properties of the
data during training. It is indicated by a bad model performance in
both training and validation and the <em>predicted</em> values deviate from
the <em>true</em> values, corresponding to a high <a class="reference internal" href="#term-bias"><span class="xref std std-term">bias</span></a>.</p>
<p><em>Undertraining</em> may occur when</p>
<ul class="simple">
<li><p>there is not enough training data</p></li>
<li><p>there is too much noise in the training data, hiding the
real characteristics and dependencies</p></li>
<li><p>training does not continue long enough</p></li>
<li><p>the model is inadequate and perhaps too simple to capture the
characteristics of the data (e.g. as when trying to fit a linear
function to datapoints following a sinus function)</p></li>
</ul>
<p>Possible strategies:</p>
<ul class="simple">
<li><p>more training data</p></li>
<li><p>cleaner training data with less noise and statistical fluctuations</p></li>
<li><p>a more sophisticated or flexible model (e.g. more parameters,
different types of layers)</p></li>
</ul>
</dd>
</dl>
</section>
<section id="v">
<h2>V<a class="headerlink" href="#v" title="Permalink to this headline">#</a></h2>
<dl class="glossary">
<dt id="term-variance">variance<a class="headerlink" href="#term-variance" title="Permalink to this term">#</a></dt><dd><p>The amount of variation in the model itself.</p>
<p>For example, in
function regression, a model may have very low bias, i.e. approximate
the given <em>true values</em> very well, but at the same time oscillate
and fluctuate wildly in between those true values. Such a model
will generalise poorly to new data, see <a class="reference internal" href="#term-overtraining"><span class="xref std std-term">overtraining</span></a>.</p>
</dd>
<dt id="term-VPC">VPC<a class="headerlink" href="#term-VPC" title="Permalink to this term">#</a></dt><dd><p>Amazon <em>Virtual Private Cloud</em></p>
</dd>
</dl>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="literature.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Links &amp; Literature</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="genindex.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Index</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
      &copy; Copyright 2021, Daniel Rauch (dmrauch).<br/>
    Last updated on August 7, 2022.<br/>
    <div class="extra_footer">
      Built with <a href="https://www.sphinx-doc.org/en/master/index.html">Sphinx</a>
using the <a href="https://sphinx-book-theme.readthedocs.io/en/latest/">Sphinx Book Theme</a>
    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>