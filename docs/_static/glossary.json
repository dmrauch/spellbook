glossary = '{"auc": "<p><em>Area Under the Curve</em></p>\\n<p>The area under a ROC curve. AUC is a metric that can be used to\\nbenchmark different models against each other, with larger values\\ncorresponding to better model performance.</p>\\n<p>See <a class=&quot;reference internal&quot; href=&quot;#term-ROC&quot;><span class=&quot;xref std std-term&quot;>ROC</span></a></p>\\n", "batch": "<p>A batch is a subset of datapoints in a dataset that is processed\\nin sequence by a neural network before the loss function is evaluated,\\nthe gradients are calculated and the model weights are updated\\nusing backprop.</p>\\n<p>The larger the batch size is, the less frequently\\nthese updates will happen and therefore, the less time each epoch will\\ntake. On the flip side, the model parameters will not be updated\\nvery often and the progress from epoch to the next may be slower.\\nChoosing smaller batch sizes on the other hand will lead to more\\ngradient evaluations, more frequent updates of the model weights and\\ntherefore a longer duration of each epoch.</p>\\n", "bias": "<p>The deviation of the predictions from the true labels in classification\\nand of the predicted values from the true values in regression</p>\\n", "binary classification": "<p>Classification problem with only two classes. These are often\\n<em>positive</em> and <em>negative</em>, e.g. when a test for a disease is performed,\\nand the <em>negative</em> class is associated with the null hypothesis, i.e.\\nthe hypothesis that is assumed to be valid until evidence of the\\ncontrary is presented.</p>\\n<p>Contrast to <a class=&quot;reference internal&quot; href=&quot;#term-multi-class-classification&quot;><span class=&quot;xref std std-term&quot;>multi-class classification</span></a>.</p>\\n", "calibration": "<p><em>Calibration</em> refers to the process of checking and correcting\\nthe <a class=&quot;reference internal&quot; href=&quot;#term-score&quot;><span class=&quot;xref std std-term&quot;>score</span></a> of a model in terms of its possible interpretation\\nas a probability. For example, in <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a>, the\\n<a class=&quot;reference internal&quot; href=&quot;#term-sigmoid&quot;><span class=&quot;xref std std-term&quot;>sigmoid</span></a>-activated model output is often interpreted as the\\nprobability that the datapoint belongs to the <em>positive</em> class.\\nHowever, the fact that the <a class=&quot;reference internal&quot; href=&quot;#term-sigmoid&quot;><span class=&quot;xref std std-term&quot;>sigmoid</span></a> function returns values\\nin [0, 1] does not yet guarantee that its values accurately quantify\\nthe probability for a datapoint to belong to a certain class. These\\nprobabilities and their relation to the model <a class=&quot;reference internal&quot; href=&quot;#term-score&quot;><span class=&quot;xref std std-term&quot;>score</span></a> are\\ndetermined during <em>calibration</em>.</p>\\n<p>The principle roughly is the following, illustrated with the example\\nof <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a> in mind:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>Choose a certain threshold for the model <a class=&quot;reference internal&quot; href=&quot;#term-score&quot;><span class=&quot;xref std std-term&quot;>score</span></a>, above which\\na datapoint is sorted into the positive category</p></li>\\n<li><p>Apply this threshold and</p>\\n<ul>\\n<li><p>average the predicted scores over all datapoints that exceed the\\nthreshold</p></li>\\n<li><p>among the datapoints exceeding the threshold, determine the\\nfraction that truly belong to the <em>positive</em> class</p></li>\\n</ul>\\n</li>\\n<li><p>Add a point to the calibration plot with the average predicted score\\non the x-axis and the fraction of true positives on the y-axis</p></li>\\n<li><p>Repeat this with a number of different thresholds</p></li>\\n</ul>\\n<p>This results in a <em>calibration curve</em>, e.g. such as the one in\\n<a class=&quot;reference external&quot; href=&quot;https://scikit-learn.org/stable/modules/calibration.html#calibration-curves&quot;>https://scikit-learn.org/stable/modules/calibration.html#calibration-curves</a>.</p>\\n", "cl": "<p><a class=&quot;reference internal&quot; href=&quot;#term-confidence-level&quot;><span class=&quot;xref std std-term&quot;>confidence level</span></a></p>\\n", "cnn": "<p><em>Convolutional Neural Network</em></p>\\n<p>A neural network containing one or more convolutional layers.</p>\\n<p>In a 2D convolutional layer, typically used for images, <span class=&quot;math notranslate nohighlight&quot;>\\\\(n_f\\\\)</span>\\ntwo-dimensional filters of size <span class=&quot;math notranslate nohighlight&quot;>\\\\(f_1 \\\\times f_2\\\\)</span> are slid\\nacross the two-dimensional data arrays of size <span class=&quot;math notranslate nohighlight&quot;>\\\\(n_1 \\\\times n_2\\\\)</span>\\nto create <span class=&quot;math notranslate nohighlight&quot;>\\\\(n_f\\\\)</span> output data arrays of size\\n<span class=&quot;math notranslate nohighlight&quot;>\\\\((n_1-f_1+1) \\\\times (n_2-f_2+1)\\\\)</span>. Analogous convolutional\\nlayers of different dimensionalities exist as well.\\nConvolutional layers are often followed by pooling layers that\\naggregate neighbouring pixels or voxels by calculating their maximum\\nor average.</p>\\n<p>By training and adjusting the filters, the neural network can\\ndistill particular patterns in the data and feed them to the\\nfollowing dense layers.</p>\\n", "confidence level": "<p>The probability of <em>not</em> making a\\n<a class=&quot;reference internal&quot; href=&quot;#term-type-1-error&quot;><span class=&quot;xref std std-term&quot;>type-1 error</span></a>, i.e. the probability of <em>not</em> wrongly rejecting\\nthe null hypothesis and therefore rightly accepting the null hypothesis.</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{confidence level} = \\\\text{CL} = 1 - \\\\alpha\\\\]</div>\\n", "data augmentation": "<p>Techniques for artificially increasing the size of the dataset</p>\\n<p>For example, in computer vision, images in the training set\\nmay be subjected to random shifts, rotations, shearing, horizontal\\nflipping, changes in brightness, contrast, saturation and other\\nproperties.</p>\\n<p>This can help to increase the model performance by allowing for\\nmore and longer training while at the same time avoiding\\n<a class=&quot;reference internal&quot; href=&quot;#term-overtraining&quot;><span class=&quot;xref std std-term&quot;>overtraining</span></a>.</p>\\n", "dropout": "<p>A <em>dropout layer</em> in a neural network randomly sets some of the values\\npassed into it from the preceding layer to zero, i.e. randomly drops or\\ndeactivates some of its inputs. The fraction of dropped nodes, usually\\ncalled <em>dropout rate</em>, is a model <a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter&quot;><span class=&quot;xref std std-term&quot;>hyperparameter</span></a>.</p>\\n<p>The original paper: G.E. Hinton et al: <em>Improving neural networks by\\npreventing co-adaptation of feature detectors</em>, <a class=&quot;reference external&quot; href=&quot;https://arxiv.org/abs/1207.0580&quot;>arXiv:1207.0580</a></p>\\n", "ebs": "<p>Amazon <em>Elastic Block Store</em></p>\\n", "ec2": "<p>Amazon <em>Elastic Cloud Compute</em>: Virtual servers</p>\\n", "ecr": "<p>Amazon <em>Elastic Container Registry</em>: Repositories for <em>Docker</em>\\ncontainers</p>\\n", "fn": "<p><em>False Negative</em>: The label of a datapoint is predicted to be\\n<em>negative</em>, but is <em>positive</em> in reality.</p>\\n<div class=&quot;admonition caution&quot;>\\n<p class=&quot;admonition-title&quot;>Caution</p>\\n<p>False negatives can be particularly dangerous as e.g. a\\npatient who really has a condition is not detected as sick and\\ntherefore is not treated.</p>\\n</div>\\n", "fp": "<p><em>False Positive</em>: The label of a datapoint is predicted to be\\n<em>positive</em>, but is <em>negative</em> in reality.</p>\\n", "fpr": "<p><em>False Positive Rate</em></p>\\n<p>Defined as</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{FPR} := \\\\frac{\\\\text{FP}}{\\\\text{TN} + \\\\text{FP}}\\n= 1 - \\\\text{specificity} \\\\approx \\\\alpha\\\\]</div>\\n<p>where <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{FP}\\\\)</span> are the false positives and <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{TN}\\\\)</span>\\nthe true negatives.</p>\\n<p>It specifies what fraction of the truly negative datapoints were\\nincorrectly classified / predicted to be positive. Therefore, it is\\nrelated to the <a class=&quot;reference internal&quot; href=&quot;#term-type-1-error&quot;><span class=&quot;xref std std-term&quot;>type-1 error</span></a> and its probability <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\alpha\\\\)</span>.</p>\\n", "hyperparameter": "<p><em>Hyperparameters</em> characterise the layout and architecture of the\\nmodel and its associated functions and algorithms. As such,\\n<em>hyperparameters</em> are not and cannot be changed\\nduring training. There are two different types of <em>hyperparameters</em>:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p><strong>model hyperparameters</strong>: e.g. the number of hidden layers in a\\nneural network, the <a class=&quot;reference internal&quot; href=&quot;#term-dropout&quot;><span class=&quot;xref std std-term&quot;>dropout</span></a> rate, the activation function\\nof a specific layer</p></li>\\n<li><p><strong>algorithm hyperparameters</strong>: e.g. the optimiser, its learning\\nrate, the batch size</p></li>\\n</ul>\\n<p><em>Hyperparameters</em> can be searched and optimised to maximise model\\nperformance. This process is called <a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter-tuning&quot;><span class=&quot;xref std std-term&quot;>hyperparameter tuning</span></a> or\\n<a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter-optimisation&quot;><span class=&quot;xref std std-term&quot;>hyperparameter optimisation</span></a>.</p>\\n<p>Contrast against <a class=&quot;reference internal&quot; href=&quot;#term-model-parameter&quot;><span class=&quot;xref std std-term&quot;>model parameter</span></a>.</p>\\n", "hyperparameter optimisation": "<p>see <a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter-tuning&quot;><span class=&quot;xref std std-term&quot;>hyperparameter tuning</span></a></p>\\n", "hyperparameter tuning": "<p>Evaluation of the achievable model performance when\\ntrying out different values for one or more hyperparameters. Normally,\\n<em>hyperparameter tuning</em> refers to automated strategies for scanning\\ndifferent <a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter&quot;><span class=&quot;xref std std-term&quot;>hyperparameter</span></a> values and ranges.</p>\\n<p>Since evaluating a single point in hyperparameter space involves\\ntraining and validating a model, <em>hyperparameter tuning</em> can be quite\\ntime-consuming and resource-intensive. Therefore, normally, not the\\nfull hyperparameter space is scanned for a model, but rather a\\nsubset.</p>\\n<p>Tuning strategies broadly fall into three basic categories:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p><strong>grid searches</strong>: All possible combinations of the selected\\nhyperparameters and their values are tried out systematically.\\nFor categorical hyperparameters, e.g. the choice of the optimiser,\\nall specified options are tried, and for continuous and ordinal\\nhyperparameters, linearly or logarithmically equidistant points\\nwithin configured ranges may be tried.</p></li>\\n<li><p><strong>random searches</strong>: Points in the configured hyperparameter space\\nare picked randomly</p></li>\\n<li><p><strong>advanced searches</strong>: Advanced searches try to make informed\\ndecisions on which hyperparameter point to evaluate next, based\\non which hyperparameter points were scanned before and how they\\nperformed. A typical strategy is Bayesian optimisation together with\\nGaussian random processes.</p></li>\\n</ul>\\n", "iam": "<p>AWS <em>Identity &amp; Access Management</em></p>\\n", "image augmentation": "<p>In <em>image augmentation</em>, transformations are applied to images\\nbefore feeding them into a model. These transformations can serve\\nto normalise the images, e.g. by rescaling them with a common\\nfactor, as well as to effectively increase the size of the datasets by\\napplying random flips, rotations, brightness changes and other\\ntransformations. While these random transformations can help protect\\nagainst <a class=&quot;reference internal&quot; href=&quot;#term-overtraining&quot;><span class=&quot;xref std std-term&quot;>overtraining</span></a>, they can also help the trained model\\nin generalising to other images. For example, this would be the\\ncase with the <em>Fashion-MNIST</em> dataset which, among other types of\\nclothes, contains shoes which are all pointing with their tips to\\nthe left.</p>\\n", "imbalanced data": "<p>When the data contain significantly more datapoints in one class than\\nthe other(s), in <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a> or\\n<a class=&quot;reference internal&quot; href=&quot;#term-multi-class-classification&quot;><span class=&quot;xref std std-term&quot;>multi-class classification</span></a>.</p>\\n<p>See <a class=&quot;reference internal&quot; href=&quot;examples/1-binary-stroke-prediction/index.html&quot;><span class=&quot;doc&quot;>Binary Classification with the Stroke Prediction Dataset</span></a></p>\\n", "kms": "<p>AWS <em>Key Management Service</em></p>\\n", "l1 regularisation": "<p>When <em>L1</em> (or <em>lasso</em>) <em>regularisation</em> is activated for a layer, a\\npenalty term <em>proportional to the sum of the absolute values</em> of the\\nweights of that layer is added to the loss function. The strength\\nof the regularisation can be adjusted by scaling the penalty term\\nwith a factor.</p>\\n", "l2 regularisation": "<p>When <em>L2</em> (or <em>ridge</em>) <em>regularisation</em> is activated for a layer, a\\npenalty term <em>quadratic in the sum</em> of the weights of that layer is\\nadded to the loss function. The strength of the regularisation can\\nbe adjusted by scaling the penalty term with a factor.</p>\\n", "lasso regularisation": "<p>see <a class=&quot;reference internal&quot; href=&quot;#term-L1-regularisation&quot;><span class=&quot;xref std std-term&quot;>L1 regularisation</span></a></p>\\n", "model parameter": "<p><em>Model parameters</em> are the parameters adjusted during training to\\nminimise the loss function and fit the model to the training data,\\ne.g. the weights of the edges between the nodes in a neural network.</p>\\n<p>Contrast against <a class=&quot;reference internal&quot; href=&quot;#term-hyperparameter&quot;><span class=&quot;xref std std-term&quot;>hyperparameter</span></a>.</p>\\n", "multi-class classification": "<p>Classification problem involving more than two classes</p>\\n<p>Contrast to <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a>.</p>\\n", "overfitting": "<p>see <a class=&quot;reference internal&quot; href=&quot;#term-overtraining&quot;><span class=&quot;xref std std-term&quot;>overtraining</span></a></p>\\n", "oversampling": "<p>Method for addressing <a class=&quot;reference internal&quot; href=&quot;#term-imbalanced-data&quot;><span class=&quot;xref std std-term&quot;>imbalanced data</span></a></p>\\n<p>See <a class=&quot;reference internal&quot; href=&quot;examples/1-binary-stroke-prediction/index.html&quot;><span class=&quot;doc&quot;>Binary Classification with the Stroke Prediction Dataset</span></a></p>\\n", "overtraining": "<p>Also called <em>overfitting</em></p>\\n<p>When the model memorises specific random fluctuations in the training\\ndata. Since the validation does not contain the exact same datapoints,\\nbut rather others with different random fluctuations, the model fails\\nto generalise to the validation data. Therefore, when <em>overtraining</em>\\noccurs, the model performance is worse during validation than in\\ntraining.</p>\\n<p>In training, the <em>predicted</em> values lie close to the <em>true</em> values,\\nbut the model fails to generalise beyond the specific datapoints,\\ncorresponding to a low <a class=&quot;reference internal&quot; href=&quot;#term-bias&quot;><span class=&quot;xref std std-term&quot;>bias</span></a> but high <a class=&quot;reference internal&quot; href=&quot;#term-variance&quot;><span class=&quot;xref std std-term&quot;>variance</span></a>.</p>\\n<p><em>Overtraining</em> may occur when</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>the model is too complex, i.e. it has too many parameters</p></li>\\n<li><p>training continues for too long on a too limited dataset</p></li>\\n</ul>\\n<p>There are several strategies aimed at avoiding <em>overtraining</em>:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>more training data</p></li>\\n<li><p>early stopping of the training, when the loss and accuracy do not\\nimprove anymore</p></li>\\n<li><p>a less complex model with fewer parameters</p></li>\\n<li><p>regularisation techniques</p>\\n<ul>\\n<li><p><a class=&quot;reference internal&quot; href=&quot;#term-dropout&quot;><span class=&quot;xref std std-term&quot;>dropout</span></a> layers</p></li>\\n<li><p><a class=&quot;reference internal&quot; href=&quot;#term-L1-regularisation&quot;><span class=&quot;xref std std-term&quot;>L1 regularisation</span></a> or <a class=&quot;reference internal&quot; href=&quot;#term-L2-regularisation&quot;><span class=&quot;xref std std-term&quot;>L2 regularisation</span></a></p></li>\\n</ul>\\n</li>\\n<li><p><a class=&quot;reference internal&quot; href=&quot;#term-data-augmentation&quot;><span class=&quot;xref std std-term&quot;>data augmentation</span></a></p></li>\\n</ul>\\n", "power": "<p>The <em>power</em> of a test or classifier quantifies its capability of\\ndetecting a <em>positive</em> result. Therefore, it is related to the\\nprobability of the <a class=&quot;reference internal&quot; href=&quot;#term-type-2-error&quot;><span class=&quot;xref std std-term&quot;>type-2 error</span></a> <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\beta\\\\)</span> by:</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{power} = 1 - \\\\beta\\\\]</div>\\n<p>See also: <a class=&quot;reference internal&quot; href=&quot;#term-TPR&quot;><span class=&quot;xref std std-term&quot;>TPR</span></a></p>\\n", "precision": "<p>Defined as</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{precision} := \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FP}}\\\\]</div>\\n<p>where <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{TP}\\\\)</span> are the true positives and <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{FP}\\\\)</span>\\nthe false positives.</p>\\n<p>It specifies what fraction of the datapoints that were\\nclassified/predicted to be <em>positive</em> are in fact truly <em>positive</em>,\\ni.e. which fraction of the <em>positive</em> classifications/predictions\\nis correct. Therefore, e.g. in the context of medical tests,\\nthe <em>precision</em> is of special interest to the tested person or\\npatient because it gives the probability for the <em>positive</em> result\\nto be actually true.</p>\\n", "recall": "<p>see <em>True Positive Rate</em> (<a class=&quot;reference internal&quot; href=&quot;#term-TPR&quot;><span class=&quot;xref std std-term&quot;>TPR</span></a>)</p>\\n", "ridge regularisation": "<p>see <a class=&quot;reference internal&quot; href=&quot;#term-L2-regularisation&quot;><span class=&quot;xref std std-term&quot;>L2 regularisation</span></a></p>\\n", "roc": "<p><em>Receiver Operator Characteristic</em></p>\\n<p>The ROC curve shows the <em>true positive rate</em> (<a class=&quot;reference internal&quot; href=&quot;#term-TPR&quot;><span class=&quot;xref std std-term&quot;>TPR</span></a>) vs. the\\n<em>false positive rate</em> (<a class=&quot;reference internal&quot; href=&quot;#term-FPR&quot;><span class=&quot;xref std std-term&quot;>FPR</span></a>) for a given model. So it\\nessentially gives the balance between type-1 and type-2 errors and\\nvisualises to what extent decreasing one will increase the other.\\nChoosing a certain threshold value of the activated classifier\\noutput (and thereby defining the rule for associating datapoints with\\nclasses) corresponds to picking a working point somewhere on a given\\nROC curve and moving the threshold value scans the ROC curve so that\\na working point with the desired balance of error rates can be picked.</p>\\n<p>The more the ROC curve extends to the top left corner, i.e. towards\\nhigh TPRs at low FPRs, the better the performance of a model.\\nTherefore, the <em>area under the curve</em> (<a class=&quot;reference internal&quot; href=&quot;#term-AUC&quot;><span class=&quot;xref std std-term&quot;>AUC</span></a>) of a ROC curve can\\nbe used to benchmark different models against each other.</p>\\n", "sample": "<p>In datascience, <em>sample</em> refers to a single datapoint.</p>\\n<p>Since I have a background in particle physics, where the term <em>sample</em>\\nusually refers to a set of generated/simulated datapoints, I tend to\\navoid it and usually prefer <em>datapoint</em>.</p>\\n<p>The vocabulary \u2018confusion matrix\u2019 that translates between data science\\nand particle physics is the following:</p>\\n<table class=&quot;table&quot;>\\n<colgroup>\\n<col style=&quot;width: 20%&quot;/>\\n<col style=&quot;width: 16%&quot;/>\\n<col style=&quot;width: 64%&quot;/>\\n</colgroup>\\n<thead>\\n<tr class=&quot;row-odd&quot;><th class=&quot;head&quot;><p>object</p></th>\\n<th class=&quot;head&quot;><p>data science</p></th>\\n<th class=&quot;head&quot;><p>particle physics</p></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr class=&quot;row-even&quot;><td><p>single entity</p></td>\\n<td><p>sample</p></td>\\n<td><p>datapoint</p></td>\\n</tr>\\n<tr class=&quot;row-odd&quot;><td><p>set of entities</p></td>\\n<td><p>dataset</p></td>\\n<td><ul class=&quot;simple&quot;>\\n<li><p>if <em>measured</em>: dataset</p></li>\\n<li><p>if <em>generated/simulated</em>: (Monte Carlo) sample</p></li>\\n</ul>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n", "score": "<p>The <em>score of a model</em> is the activated output of a model, e.g. the\\nactivated output of the last layer in a neural network.</p>\\n<p>The unactivated outputs are called <em>logits</em>.</p>\\n<p>Commonly used activation functions are</p>\\n<ul class=&quot;simple&quot;>\\n<li><p><a class=&quot;reference internal&quot; href=&quot;#term-sigmoid&quot;><span class=&quot;xref std std-term&quot;>sigmoid</span></a> activation in <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a></p></li>\\n<li><p><a class=&quot;reference internal&quot; href=&quot;#term-softmax&quot;><span class=&quot;xref std std-term&quot;>softmax</span></a> activation in <a class=&quot;reference internal&quot; href=&quot;#term-multi-class-classification&quot;><span class=&quot;xref std std-term&quot;>multi-class classification</span></a></p></li>\\n</ul>\\n", "sensitivity": "<p>see <em>True Positive Rate</em> (<a class=&quot;reference internal&quot; href=&quot;#term-TPR&quot;><span class=&quot;xref std std-term&quot;>TPR</span></a>)</p>\\n", "sigmoid": "<p>Sigmoid functions follow a characteristic \u2018S\u2019-shape. In machine\\nlearning, <em>sigmoid activation</em> usually refers to using the\\n<em>logistic function</em></p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[f(x) = \\\\frac{1}{1 + e^{-x}}\\\\]</div>\\n<p>as the activation function.</p>\\n<p>Since the <em>sigmoid function</em> maps all real numbers to\\nthe interval (0, 1), <em>sigmoid activation</em> is typically used in\\n<a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a>, with outputs close to 0 associated to\\none category and outputs close to 1 to the other. The sigmoid-\\nactivated network output is also often interpreted as the probability\\nof a datapoint to belong to the second class, but this interpretation\\nhas to be taken with a grain of salt, see <a class=&quot;reference internal&quot; href=&quot;#term-calibration&quot;><span class=&quot;xref std std-term&quot;>calibration</span></a>.</p>\\n", "softmax": "<p>The <em>softmax</em> function is typically used as the activation function\\nin <a class=&quot;reference internal&quot; href=&quot;#term-multi-class-classification&quot;><span class=&quot;xref std std-term&quot;>multi-class classification</span></a> problems with one-hot\\nencoded labels. It is defined as</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\sigma(\\\\vec{x})_i = \\\\frac{e^{x_i}}{\\\\sum_{j=1}^n e^{x_j}}\\\\]</div>\\n<p>Each of the <span class=&quot;math notranslate nohighlight&quot;>\\\\(n\\\\)</span> target classes corresponds to one entry in\\nthe classification vector <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\vec{x}\\\\)</span> and the <em>softmax</em> function\\nprovides a mapping <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\mathbb{R}^n \\\\to [0,1]^n\\\\)</span>. Furthermore,\\nit provides a normalisation such that the activated entries of the\\nclassification vector sum up to unity, i.e.\\n<span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\sum_{i=1}^n \\\\sigma(\\\\vec{x})_i = 1\\\\)</span>.\\nThis is what is naturally expected for discreet probabilities.\\nHowever, as long as a classifier is not calibrated, it cannot be\\nguaranteed that the activated output of the last layer gives the\\nprobabilities for a datapoint to belong to each of the involved\\ntarget classes.</p>\\n", "specificity": "<p>Defined as</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{specificity} := \\\\frac{\\\\text{TN}}{\\\\text{TN} + \\\\text{FP}}\\n= 1 - \\\\text{FPR}\\\\]</div>\\n<p>where <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{TN}\\\\)</span> are the true negatives and <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{FP}\\\\)</span>\\nthe false positives.</p>\\n<p>It specifies what fraction of the truly <em>negative</em> datapoints was\\ncorrectly classified/predicted to be <em>negative</em>. Therefore, the\\n<em>specificity</em> is related to the <a class=&quot;reference internal&quot; href=&quot;#term-FPR&quot;><span class=&quot;xref std std-term&quot;>FPR</span></a>.</p>\\n", "testing": "<p>The determination of the <em>unbiased</em> model performance.</p>\\n<p>To this end, the full dataset available during model design, training\\nand development is split up into three distinct parts:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>the <em>training</em> dataset</p></li>\\n<li><p>the <em>validation</em> / <em>hold-out cross-validation</em> or\\n<em>development</em> dataset and</p></li>\\n<li><p>the <em>test</em> dataset</p></li>\\n</ul>\\n<p>While the model parameters are adjusted on the <em>training</em> dataset, the\\nperformance of the model during the development phase is estimated from\\nthe <em>validation</em> dataset. Between the training runs, the\\nhyperparameters are changed so as to maximise the performance metrics\\nevaluated from the <em>validation</em> dataset. Finally, at the end of the\\ndevelopment phase, a specific model and a set of hyperparameters is\\nchosen and afterwards, the model performance is evaluated based on the\\n<em>test</em> dataset. This is an unbiased estimate since the <em>test</em> data\\nwere never previously used to make choices regarding the model.</p>\\n<p>Many times, when getting a proper unbiased estimate of the model\\nperformance is not crucial, no separate testing is performed. In such\\ncases, the model performance is simply quantified with the validation\\nresults. In practice, this validation stage is then often referred to\\nas \u2018testing\u2019.</p>\\n", "tn": "<p><em>True Negative</em>: The label of a datapoint is predicted to be\\n<em>negative</em> and also is <em>positive</em> in reality</p>\\n", "tp": "<p><em>True Positive</em>: The label of a datapoint is predicted to be\\n<em>positive</em> and also is <em>positive</em> in reality</p>\\n", "tpr": "<p><em>True Positive Rate</em></p>\\n<p>Defined as</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\text{TPR} := \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FN}}\\n= \\\\text{sensitivity}\\n= \\\\text{recall} \\\\approx 1 - \\\\beta\\\\]</div>\\n<p>where <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{TP}\\\\)</span> are the true positives and <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\text{FN}\\\\)</span>\\nthe false negatives.</p>\\n<p>It specifies what fraction of the truly positive datapoints were\\ncorrectly classified / predicted to be positive. Therefore, it is\\nrelated to the <a class=&quot;reference internal&quot; href=&quot;#term-type-2-error&quot;><span class=&quot;xref std std-term&quot;>type-2 error</span></a> and its probability <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\beta\\\\)</span>,\\nor, more specifically the <a class=&quot;reference internal&quot; href=&quot;#term-power&quot;><span class=&quot;xref std std-term&quot;>power</span></a> <span class=&quot;math notranslate nohighlight&quot;>\\\\(1 - \\\\beta\\\\)</span>.</p>\\n", "type-1 error": "<p>The error of wrongly rejecting the null hypothesis and accepting the\\nalternative hypothesis. Its probability is denoted with <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\alpha\\\\)</span>:</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\alpha := P(\\\\text{type-1 error})\\\\]</div>\\n<p>It is related to the <a class=&quot;reference internal&quot; href=&quot;#term-confidence-level&quot;><span class=&quot;xref std std-term&quot;>confidence level</span></a> (CL) by</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\alpha = 1 - \\\\text{confidence level}\\\\]</div>\\n<p>In <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a>, where the null hypothesis is usually\\ntaken to be</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>a negative test</p></li>\\n<li><p>the patient is healthy</p></li>\\n<li><p>the absence of new physics effects and the validity of the currently\\nestablished model</p></li>\\n</ul>\\n<p>or a similarly <em>normal</em> situation, making type-1 errors results in\\n<em>false positives</em> (<a class=&quot;reference internal&quot; href=&quot;#term-FP&quot;><span class=&quot;xref std std-term&quot;>FP</span></a>).</p>\\n", "type-2 error": "<p>The error of wrongly accepting the null hypothesis and rejecting the\\nalternative hypothesis. Its probability is denoted with <span class=&quot;math notranslate nohighlight&quot;>\\\\(\\\\beta\\\\)</span>:</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\beta := P(\\\\text{type-2 error})\\\\]</div>\\n<p>It is related to the <a class=&quot;reference internal&quot; href=&quot;#term-power&quot;><span class=&quot;xref std std-term&quot;>power</span></a> by</p>\\n<div class=&quot;math notranslate nohighlight&quot;>\\n\\\\[\\\\beta = 1 - \\\\text{power}\\\\]</div>\\n<p>In <a class=&quot;reference internal&quot; href=&quot;#term-binary-classification&quot;><span class=&quot;xref std std-term&quot;>binary classification</span></a>, where the null hypothesis is usually\\ntaken to be</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>a negative test</p></li>\\n<li><p>the patient is healthy</p></li>\\n<li><p>the absence of new physics effects and the validity of the currently\\nestablished model</p></li>\\n</ul>\\n<p>or a similarly <em>normal</em> situation, making type-2 errors results in\\n<em>false negatives</em> (<a class=&quot;reference internal&quot; href=&quot;#term-FN&quot;><span class=&quot;xref std std-term&quot;>FN</span></a>).</p>\\n", "underfitting": "<p>see <a class=&quot;reference internal&quot; href=&quot;#term-undertraining&quot;><span class=&quot;xref std std-term&quot;>undertraining</span></a></p>\\n", "undertraining": "<p>Also called <em>underfitting</em>.</p>\\n<p>When the model fails to learn the characteristic properties of the\\ndata during training. It is indicated by a bad model performance in\\nboth training and validation and the <em>predicted</em> values deviate from\\nthe <em>true</em> values, corresponding to a high <a class=&quot;reference internal&quot; href=&quot;#term-bias&quot;><span class=&quot;xref std std-term&quot;>bias</span></a>.</p>\\n<p><em>Undertraining</em> may occur when</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>there is not enough training data</p></li>\\n<li><p>there is too much noise in the training data, hiding the\\nreal characteristics and dependencies</p></li>\\n<li><p>training does not continue long enough</p></li>\\n<li><p>the model is inadequate and perhaps too simple to capture the\\ncharacteristics of the data (e.g. as when trying to fit a linear\\nfunction to datapoints following a sinus function)</p></li>\\n</ul>\\n<p>Possible strategies:</p>\\n<ul class=&quot;simple&quot;>\\n<li><p>more training data</p></li>\\n<li><p>cleaner training data with less noise and statistical fluctuations</p></li>\\n<li><p>a more sophisticated or flexible model (e.g. more parameters,\\ndifferent types of layers)</p></li>\\n</ul>\\n", "variance": "<p>The amount of variation in the model itself.</p>\\n<p>For example, in\\nfunction regression, a model may have very low bias, i.e. approximate\\nthe given <em>true values</em> very well, but at the same time oscillate\\nand fluctuate wildly in between those true values. Such a model\\nwill generalise poorly to new data, see <a class=&quot;reference internal&quot; href=&quot;#term-overtraining&quot;><span class=&quot;xref std std-term&quot;>overtraining</span></a>.</p>\\n", "vpc": "<p>Amazon <em>Virtual Private Cloud</em></p>\\n"}';